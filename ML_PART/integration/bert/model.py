from transformers import AutoTokenizer, AutoModelForTokenClassification
from huggingface_hub import snapshot_download
import logging
import os
import torch
from TorchCRF import CRF

fine_tuned_model = None
loaded_tokenizer = None
logger = logging.getLogger(__name__)


class NERModelWithCRF(torch.nn.Module):
    def __init__(self, num_labels, model_checkpoint="DeepPavlov/rubert-base-cased"):
        super().__init__()
        self.bert = AutoModelForTokenClassification.from_pretrained(
            model_checkpoint,
            num_labels=num_labels
        )
        self.crf = CRF(num_labels)
        self.model_checkpoint = model_checkpoint

    def forward(self, input_ids, attention_mask, labels=None):
        outputs = self.bert(input_ids, attention_mask=attention_mask)
        emissions = outputs.logits

        if labels is not None:
            labels_crf = labels.clone()
            labels_crf[labels == -100] = 0
            loss = -self.crf(emissions, labels_crf, mask=attention_mask.type(torch.uint8))
            return loss
        else:
            return self.crf.viterbi_decode(emissions, mask=attention_mask.type(torch.uint8))

    def get_emissions(self, input_ids, attention_mask):
        with torch.no_grad():
            outputs = self.bert(input_ids, attention_mask=attention_mask)
            return outputs.logits

    def save_pretrained(self, save_directory):
        self.bert.save_pretrained(save_directory)
        crf_path = os.path.join(save_directory, "crf_layer.pt")
        torch.save(self.crf.state_dict(), crf_path)

        metadata = {
            "model_type": "bert-crf",
            "model_checkpoint": self.model_checkpoint,
            "num_labels": self.bert.config.num_labels,
            "crf_layer": "crf_layer.pt"
        }

        import json
        with open(os.path.join(save_directory, "model_metadata.json"), "w") as f:
            json.dump(metadata, f, indent=2)

        print(f"‚úÖ –ü–æ–ª–Ω–∞—è –º–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –≤: {save_directory}")

    @classmethod
    def from_pretrained(cls, save_directory, device=None):
        if device is None:
            device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

        import json
        with open(os.path.join(save_directory, "model_metadata.json"), "r") as f:
            metadata = json.load(f)

        model = cls(
            num_labels=metadata["num_labels"],
            model_checkpoint=metadata["model_checkpoint"]
        )

        model.bert = AutoModelForTokenClassification.from_pretrained(save_directory)

        # –ò—Å–ø—Ä–∞–≤–ª–µ–Ω–∏–µ: –∑–∞–≥—Ä—É–∂–∞–µ–º CRF —Å –ø—Ä–∞–≤–∏–ª—å–Ω—ã–º —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ–º
        crf_path = os.path.join(save_directory, "crf_layer.pt")
        model.crf.load_state_dict(torch.load(crf_path, map_location=device))

        # –ü–µ—Ä–µ–º–µ—â–∞–µ–º –≤—Å—é –º–æ–¥–µ–ª—å –Ω–∞ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ
        model.to(device)
        return model

def token_labels_to_char_spans(offsets, token_labels):
    """
    offsets: list of (start,end)
    token_labels: list like ['B-BRAND','I-BRAND','O',...]
    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–ø–∏—Å–æ–∫ char-spans –≤ —Ñ–æ—Ä–º–∞—Ç–µ:
       [(start,end,'B-BRAND'), ..., (start,end,'O'), ...]
    –ü—Ä–∞–≤–∏–ª–∞:
      - Non-O spans –≤–æ–∑–≤—Ä–∞—â–∞—é—Ç—Å—è –∫–∞–∫ –µ–¥–∏–Ω—Å—Ç–≤–µ–Ω–Ω—ã–π B-<TYPE> —Å–ø–∞–Ω (–Ω–∞—á–∞–ª–æ => 'B-', –≤–Ω—É—Ç—Ä–∏ => –æ–±—ä–µ–¥–∏–Ω—è–µ—Ç—Å—è)
      - O-—Å–ø–∞–Ω—ã –≤–æ–∑–≤—Ä–∞—â–∞—é—Ç—Å—è –∫–∞–∫ 'O'
      - –û–±—ä–µ–¥–∏–Ω—è–µ–º —Ç–æ–∫–µ–Ω—ã –≤ –æ–¥–∏–Ω char-span —Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ —Å–º–µ–∂–Ω—ã (next.start == cur.end).
    """
    spans = []
    cur = None  # [start, end, base_label or 'O']
    for (off, lab) in zip(offsets, token_labels):
        t_s, t_e = off
        if t_s == t_e:
            # skip special tokens
            continue
        if lab == "O":
            if cur is None:
                cur = [t_s, t_e, "O"]
            else:
                if cur[2] == "O" and t_s == cur[1]:
                    # extend contiguous O span
                    cur[1] = t_e
                else:
                    # push previous and start new O span
                    spans.append((cur[0], cur[1], "B-" + cur[2] if cur[2] != "O" else "O") if cur[2] != "O" else (cur[0], cur[1], "O"))
                    cur = [t_s, t_e, "O"]
        else:
            # labels like B-X or I-X (robust to plain X)
            if lab.startswith("B-"):
                base = lab.split("-", 1)[1]
                if cur is not None:
                    # push previous
                    spans.append((cur[0], cur[1], "B-" + cur[2] if cur[2] != "O" else "O") if cur[2] != "O" else (cur[0], cur[1], "O"))
                cur = [t_s, t_e, base]
            elif lab.startswith("I-"):
                base = lab.split("-", 1)[1]
                if cur is not None and cur[2] == base and t_s == cur[1]:
                    cur[1] = t_e
                else:
                    # I- without B- : –Ω–∞—á–∏–Ω–∞–µ–º –Ω–æ–≤—ã–π span (robust)
                    if cur is not None:
                        spans.append((cur[0], cur[1], "B-" + cur[2] if cur[2] != "O" else "O") if cur[2] != "O" else (cur[0], cur[1], "O"))
                    cur = [t_s, t_e, base]
            else:
                # plain label like 'TYPE' -> treat as B-<TYPE>
                base = lab
                if cur is not None:
                    spans.append((cur[0], cur[1], "B-" + cur[2] if cur[2] != "O" else "O") if cur[2] != "O" else (cur[0], cur[1], "O"))
                cur = [t_s, t_e, base]

    if cur is not None:
        if cur[2] == "O":
            spans.append((cur[0], cur[1], "O"))
        else:
            spans.append((cur[0], cur[1], "B-" + cur[2]))
    return spans

class HFWrapper:
    def __init__(self, model, tokenizer, id2label):
        self.model = model
        self.tokenizer = tokenizer
        self.id2label = id2label  # –î–æ–±–∞–≤–ª—è–µ–º id2label –¥–ª—è –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏—è ID –≤ –º–µ—Ç–∫–∏

    def __call__(self, text):
        class Doc:
            def __init__(self, ents):
                self.ents = ents

        class Ent:
            def __init__(self, start, end, label):
                self.start_char = start
                self.end_char = end
                self.label_ = label

        # –¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è —Å truncation –∏ max_length (–∏–∑ CONFIG)
        tokenized = self.tokenizer(
            [text],
            padding=True,
            truncation=True,
            max_length=128,  # –ò–∑ CONFIG["max_length"]
            return_tensors="pt",
            return_offsets_mapping=True
        )
        input_ids = tokenized["input_ids"].to(self.model.bert.device)
        attention_mask = tokenized["attention_mask"].to(self.model.bert.device)

        # –ò–Ω—Ñ–µ—Ä–µ–Ω—Å –º–æ–¥–µ–ª–∏
        with torch.no_grad():
            pred = self.model(input_ids, attention_mask)[0]  # –°–ø–∏—Å–æ–∫ ID –º–µ—Ç–æ–∫ –æ—Ç viterbi_decode

        # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º ID –º–µ—Ç–æ–∫ –≤ —Ç–µ–∫—Å—Ç–æ–≤—ã–µ –º–µ—Ç–∫–∏
        token_labels = [self.id2label[id] for id in pred]

        # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –≤ —Å–ø–∞–Ω—ã —Å –ø–æ–º–æ—â—å—é token_labels_to_char_spans
        spans = token_labels_to_char_spans(tokenized["offset_mapping"][0].tolist(), token_labels)

        # –°–æ–∑–¥–∞—ë–º entities
        ents = [Ent(s, e, l) for s, e, l in spans]
        return Doc(ents)



def setup_hf_login(token=None):
    try:
        from huggingface_hub import login
        if token is None:
            token = os.getenv("HUGGINGFACE_HUB_TOKEN")
        if token:
            login(token=token)
            print("‚úÖ –ê–≤—Ç–æ—Ä–∏–∑–∞—Ü–∏—è HF –Ω–∞—Å—Ç—Ä–æ–µ–Ω–∞")
            return True
        else:
            print("‚ö†Ô∏è –¢–æ–∫–µ–Ω HF –Ω–µ –Ω–∞–π–¥–µ–Ω")
            return False
    except ImportError:
        print("‚ùå huggingface_hub –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω. –£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ: pip install huggingface_hub")
        return False
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –∞–≤—Ç–æ—Ä–∏–∑–∞—Ü–∏–∏ HF: {e}")
        return False


def load_bert_from_hf(repo_name, token=None, device=None):
    try:
        if device is None:
            device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

        if not setup_hf_login(token):
            return None, None, None

        local_dir = snapshot_download(repo_id=repo_name, token=token)

        # –ò—Å–ø—Ä–∞–≤–ª–µ–Ω–∏–µ: –ø–µ—Ä–µ–¥–∞–µ–º device –≤ from_pretrained
        model = NERModelWithCRF.from_pretrained(local_dir, device=device)
        model.eval()

        tokenizer = AutoTokenizer.from_pretrained(local_dir)

        import json
        with open(os.path.join(local_dir, "training_config.json"), "r") as f:
            config = json.load(f)

        print(f"‚úÖ BERT –º–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞ –∏–∑: {repo_name}")
        return model, tokenizer, config

    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ BERT –º–æ–¥–µ–ª–∏: {e}")
        import traceback
        traceback.print_exc()  # –î–æ–±–∞–≤–ª—è–µ–º –ø–æ–ª–Ω—É—é —Ç—Ä–∞—Å—Å–∏—Ä–æ–≤–∫—É –¥–ª—è –æ—Ç–ª–∞–¥–∫–∏
        return None, None, None


def initialize(repo_name: str, token: str) -> None:
    global fine_tuned_model
    global loaded_tokenizer
    logger.info(f"Begin initialize model from repo: {repo_name}")

    # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ –∑–∞—Ä–∞–Ω–µ–µ
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print(f"üöÄ –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ: {device}")

    fine_tuned_model, loaded_tokenizer, _ = load_bert_from_hf(
        repo_name,
        token=token,
        device=device
    )

    # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ –º–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∑–∏–ª–∞—Å—å
    if fine_tuned_model is not None and loaded_tokenizer is not None:
        logger.info("‚úÖ Model initialized successfully")
        print("‚úÖ Model initialized successfully")
    else:
        logger.error("‚ùå Failed to initialize model")
        print("‚ùå Failed to initialize model")


def predict(query: str) -> list[tuple[int, int, str]]:
    global fine_tuned_model
    global loaded_tokenizer

    # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ –º–æ–¥–µ–ª—å –∏ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä –∑–∞–≥—Ä—É–∂–µ–Ω—ã
    if fine_tuned_model is None or loaded_tokenizer is None:
        print("‚ùå –ú–æ–¥–µ–ª—å –Ω–µ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–∞. –°–Ω–∞—á–∞–ª–∞ –≤—ã–∑–æ–≤–∏—Ç–µ initialize()")
        return []

    print(f"üîç –¢–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä: {type(loaded_tokenizer)}, –ú–æ–¥–µ–ª—å: {type(fine_tuned_model)}")

    wrapper = HFWrapper(
        fine_tuned_model,
        loaded_tokenizer,
        id2label={i: label for i, label in enumerate([
            "O", "B-TYPE", "I-TYPE", "B-BRAND", "I-BRAND",
            "B-VOLUME", "I-VOLUME", "B-PERCENT", "I-PERCENT"
        ])}
    )
    doc = wrapper(query)
    return [(ent.start_char, ent.end_char, ent.label_) for ent in doc.ents]
