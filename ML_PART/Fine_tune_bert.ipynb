{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0jU5kuXfijpT","executionInfo":{"status":"ok","timestamp":1759179235926,"user_tz":-180,"elapsed":6485,"user":{"displayName":"–ê–ª–µ–∫—Å–∞–Ω–¥—Ä –§–µ–¥–æ—Ä–æ–≤","userId":"11558309455709398811"}},"outputId":"66e91234-c998-40d7-edd0-8a339ea89998"},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n","/content/drive/MyDrive/Colab Notebooks\n","–í–≤–µ–¥–∏ GitHub PAT —Ç–æ–∫–µ–Ω: ¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑\n","–ó–∞–Ω–æ–≤–æ —Å–∫–ª–æ–Ω–∏—Ä–æ–≤–∞–ª–∏ —Ä–µ–ø—É\n","fatal: destination path 'entities-extraction-x5' already exists and is not an empty directory.\n","/content/drive/MyDrive/Colab Notebooks/entities-extraction-x5/ML_PART\n","‚úÖ –í—Å—ë –≥–æ—Ç–æ–≤–æ! –†–∞–±–æ—á–∞—è –ø–∞–ø–∫–∞: /content/drive/MyDrive/Colab Notebooks/entities-extraction-x5/ML_PART\n"]}],"source":["from google.colab import drive\n","import getpass, os\n","\n","# === –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ø—Ä–æ–µ–∫—Ç–∞ ===\n","USER = \"tokarevdr\"   # —Ç–≤–æ–π GitHub username\n","REPO = \"entities-extraction-x5\"            # –Ω–∞–∑–≤–∞–Ω–∏–µ —Ä–µ–ø–æ–∑–∏—Ç–æ—Ä–∏—è\n","EMAIL = \"fedorov.alexander.04@gmail.com\"    # —Ç–≤–æ—è –ø–æ—á—Ç–∞ –¥–ª—è git\n","NAME = \"Alexander\"           # —Ç–≤–æ—ë –∏–º—è –¥–ª—è git\n","# === –ü–æ–¥–∫–ª—é—á–µ–Ω–∏–µ Google Drive ===\n","drive.mount('/content/drive')\n","PROJECTS_DIR = \"/content/drive/MyDrive/Colab Notebooks\"\n","%cd $PROJECTS_DIR\n","# === GitHub –∞–≤—Ç–æ—Ä–∏–∑–∞—Ü–∏—è ===\n","token = getpass.getpass('–í–≤–µ–¥–∏ GitHub PAT —Ç–æ–∫–µ–Ω: ')\n","os.environ[\"GITHUB_TOKEN\"] = token\n","\n","\n","# === –ü—Ä–æ–≤–µ—Ä—è–µ–º: –µ—Å–ª–∏ —Ä–µ–ø–æ–∑–∏—Ç–æ—Ä–∏–π –µ—â—ë –Ω–µ —Å–∫–∞—á–∞–Ω, –∫–ª–æ–Ω–∏—Ä—É–µ–º ===\n","if not os.path.exists(f\"{PROJECTS_DIR}/{REPO}/ML PART\"):\n","    print('–ó–∞–Ω–æ–≤–æ —Å–∫–ª–æ–Ω–∏—Ä–æ–≤–∞–ª–∏ —Ä–µ–ø—É')\n","    !git clone https://{USER}:{os.environ[\"GITHUB_TOKEN\"]}@github.com/{USER}/{REPO}.git\n","# === –ü–µ—Ä–µ—Ö–æ–¥–∏–º –≤ –ø–∞–ø–∫—É –ø—Ä–æ–µ–∫—Ç–∞ ===\n","%cd {REPO}/{'ML_PART'}\n","\n","# === –ù–∞—Å—Ç—Ä–æ–π–∫–∞ Git ===\n","!git config --global user.email \"{EMAIL}\"\n","!git config --global user.name \"{NAME}\"\n","!git remote set-url origin https://{USER}:{os.environ[\"GITHUB_TOKEN\"]}@github.com/{USER}/{REPO}.git\n","\n","print(\"‚úÖ –í—Å—ë –≥–æ—Ç–æ–≤–æ! –†–∞–±–æ—á–∞—è –ø–∞–ø–∫–∞:\", os.getcwd())\n"]},{"cell_type":"code","source":[],"metadata":{"id":"_69974-nHwLK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# –£—Å—Ç–∞–Ω–æ–≤–∫–∞ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–µ–π\n","!pip install -r requirements_bert.txt"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"43TOp5KPoqEB","executionInfo":{"status":"ok","timestamp":1759180014588,"user_tz":-180,"elapsed":19465,"user":{"displayName":"–ê–ª–µ–∫—Å–∞–Ω–¥—Ä –§–µ–¥–æ—Ä–æ–≤","userId":"11558309455709398811"}},"outputId":"709889d4-b3a9-455f-b0f5-1cc842e5c67c"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: torch==2.8.0 in /usr/local/lib/python3.12/dist-packages (from -r requirements_bert.txt (line 2)) (2.8.0+cu126)\n","Collecting transformers==4.35.2 (from -r requirements_bert.txt (line 3))\n","  Downloading transformers-4.35.2-py3-none-any.whl.metadata (123 kB)\n","\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m123.5/123.5 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting huggingface_hub==0.20.3 (from -r requirements_bert.txt (line 6))\n","  Downloading huggingface_hub-0.20.3-py3-none-any.whl.metadata (12 kB)\n","Collecting accelerate==0.24.1 (from -r requirements_bert.txt (line 7))\n","  Downloading accelerate-0.24.1-py3-none-any.whl.metadata (18 kB)\n","Collecting tokenizers==0.15.0 (from -r requirements_bert.txt (line 8))\n","  Downloading tokenizers-0.15.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n","Collecting datasets==2.14.7 (from -r requirements_bert.txt (line 9))\n","  Downloading datasets-2.14.7-py3-none-any.whl.metadata (19 kB)\n","Collecting pandas==2.1.4 (from -r requirements_bert.txt (line 12))\n","  Downloading pandas-2.1.4-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (18 kB)\n","Collecting numpy==1.24.3 (from -r requirements_bert.txt (line 13))\n","  Downloading numpy-1.24.3.tar.gz (10.9 MB)\n","\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m10.9/10.9 MB\u001b[0m \u001b[31m82.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n","  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n","  \n","  \u001b[31m√ó\u001b[0m \u001b[32mGetting requirements to build wheel\u001b[0m did not run successfully.\n","  \u001b[31m‚îÇ\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n","  \u001b[31m‚ï∞‚îÄ>\u001b[0m See above for output.\n","  \n","  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n","  Getting requirements to build wheel ... \u001b[?25l\u001b[?25herror\n","\u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n","\n","\u001b[31m√ó\u001b[0m \u001b[32mGetting requirements to build wheel\u001b[0m did not run successfully.\n","\u001b[31m‚îÇ\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n","\u001b[31m‚ï∞‚îÄ>\u001b[0m See above for output.\n","\n","\u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n"]}]},{"cell_type":"code","source":["! pip install --upgrade onnxruntime"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"WKM-_g4rH0SC","executionInfo":{"status":"ok","timestamp":1759180063037,"user_tz":-180,"elapsed":6747,"user":{"displayName":"–ê–ª–µ–∫—Å–∞–Ω–¥—Ä –§–µ–¥–æ—Ä–æ–≤","userId":"11558309455709398811"}},"outputId":"17a53c7c-ddf7-491d-b032-759ebe779a45"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: onnxruntime in /usr/local/lib/python3.12/dist-packages (1.23.0)\n","Requirement already satisfied: coloredlogs in /usr/local/lib/python3.12/dist-packages (from onnxruntime) (15.0.1)\n","Requirement already satisfied: flatbuffers in /usr/local/lib/python3.12/dist-packages (from onnxruntime) (25.2.10)\n","Requirement already satisfied: numpy>=1.21.6 in /usr/local/lib/python3.12/dist-packages (from onnxruntime) (2.0.2)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from onnxruntime) (25.0)\n","Requirement already satisfied: protobuf in /usr/local/lib/python3.12/dist-packages (from onnxruntime) (5.29.5)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.12/dist-packages (from onnxruntime) (1.13.3)\n","Requirement already satisfied: humanfriendly>=9.1 in /usr/local/lib/python3.12/dist-packages (from coloredlogs->onnxruntime) (10.0)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy->onnxruntime) (1.3.0)\n"]}]},{"cell_type":"code","source":["from google.colab import drive\n","import getpass, os, json, random, time\n","import numpy as np\n","import pandas as pd\n","import torch\n","import matplotlib.pyplot as plt\n","from sklearn.model_selection import train_test_split, KFold\n","from torch.utils.data import Dataset, DataLoader\n","from torch.optim import AdamW\n","\n","# –ò–º–ø–æ—Ä—Ç—ã transformers —Å –æ–±—Ä–∞–±–æ—Ç–∫–æ–π –æ—à–∏–±–æ–∫\n","try:\n","    from transformers import AutoTokenizer, AutoModelForTokenClassification, get_scheduler\n","    print(\"‚úÖ Transformers —É—Å–ø–µ—à–Ω–æ –∏–º–ø–æ—Ä—Ç–∏—Ä–æ–≤–∞–Ω—ã\")\n","except ImportError as e:\n","    print(f\"‚ùå –û—à–∏–±–∫–∞ –∏–º–ø–æ—Ä—Ç–∞ transformers: {e}\")\n","    !pip install transformers==4.35.2\n","    from transformers import AutoTokenizer, AutoModelForTokenClassification, get_scheduler\n","\n","try:\n","    from TorchCRF import CRF\n","    print(\"‚úÖ TorchCRF —É—Å–ø–µ—à–Ω–æ –∏–º–ø–æ—Ä—Ç–∏—Ä–æ–≤–∞–Ω\")\n","except ImportError as e:\n","    print(f\"‚ùå –û—à–∏–±–∫–∞ –∏–º–ø–æ—Ä—Ç–∞ TorchCRF: {e}\")\n","    !pip install torchcrf==1.2.0\n","    from TorchCRF import CRF\n","\n","import ast\n","import traceback\n","from module import calculate_ner_metrics, calculate_macro_f1, process_submission_bert, \\\n","                  setup_hf_login, save_bert_to_hf, load_bert_from_hf, list_my_repos, check_repo_exists\n","from torch.nn.utils.rnn import pad_sequence"],"metadata":{"id":"cU-8Ca9forrD","executionInfo":{"status":"error","timestamp":1759180097268,"user_tz":-180,"elapsed":34225,"user":{"displayName":"–ê–ª–µ–∫—Å–∞–Ω–¥—Ä –§–µ–¥–æ—Ä–æ–≤","userId":"11558309455709398811"}},"colab":{"base_uri":"https://localhost:8080/","height":1000},"outputId":"866e209d-e929-45d2-c07d-ec6804b918fb"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["‚ùå –û—à–∏–±–∫–∞ –∏–º–ø–æ—Ä—Ç–∞ transformers: cannot import name 'GenerationMixin' from 'transformers.generation' (/usr/local/lib/python3.12/dist-packages/transformers/generation/__init__.py)\n","Collecting transformers==4.35.2\n","  Using cached transformers-4.35.2-py3-none-any.whl.metadata (123 kB)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from transformers==4.35.2) (3.19.1)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.12/dist-packages (from transformers==4.35.2) (0.35.0)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from transformers==4.35.2) (2.0.2)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from transformers==4.35.2) (25.0)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers==4.35.2) (6.0.2)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers==4.35.2) (2024.11.6)\n","Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from transformers==4.35.2) (2.32.4)\n","Collecting tokenizers<0.19,>=0.14 (from transformers==4.35.2)\n","  Downloading tokenizers-0.15.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n","Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.12/dist-packages (from transformers==4.35.2) (0.6.2)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.12/dist-packages (from transformers==4.35.2) (4.67.1)\n","Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers==4.35.2) (2025.3.0)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers==4.35.2) (4.15.0)\n","Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers==4.35.2) (1.1.10)\n","Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->transformers==4.35.2) (3.4.3)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->transformers==4.35.2) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->transformers==4.35.2) (2.5.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->transformers==4.35.2) (2025.8.3)\n","Downloading transformers-4.35.2-py3-none-any.whl (7.9 MB)\n","\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m7.9/7.9 MB\u001b[0m \u001b[31m35.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading tokenizers-0.15.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.6 MB)\n","\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m61.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: tokenizers, transformers\n","  Attempting uninstall: tokenizers\n","    Found existing installation: tokenizers 0.22.0\n","    Uninstalling tokenizers-0.22.0:\n","      Successfully uninstalled tokenizers-0.22.0\n","  Attempting uninstall: transformers\n","    Found existing installation: transformers 4.56.1\n","    Uninstalling transformers-4.56.1:\n","      Successfully uninstalled transformers-4.56.1\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","sentence-transformers 5.1.0 requires transformers<5.0.0,>=4.41.0, but you have transformers 4.35.2 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0mSuccessfully installed tokenizers-0.15.2 transformers-4.35.2\n"]},{"output_type":"display_data","data":{"application/vnd.colab-display-data+json":{"pip_warning":{"packages":["tokenizers","transformers"]},"id":"b216cd1509df4de98d18b7ae936757e1"}},"metadata":{}},{"output_type":"error","ename":"ImportError","evalue":"cannot import name 'is_torch_tpu_available' from 'transformers.utils' (/usr/local/lib/python3.12/dist-packages/transformers/utils/__init__.py)","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)","\u001b[0;32m/tmp/ipython-input-3563112187.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m     \u001b[0;32mfrom\u001b[0m \u001b[0mtransformers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mAutoTokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAutoModelForTokenClassification\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mget_scheduler\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"‚úÖ Transformers —É—Å–ø–µ—à–Ω–æ –∏–º–ø–æ—Ä—Ç–∏—Ä–æ–≤–∞–Ω—ã\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/utils/import_utils.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/utils/import_utils.py\u001b[0m in \u001b[0;36m_get_module\u001b[0;34m(self, module_name)\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/utils/import_utils.py\u001b[0m in \u001b[0;36m_get_module\u001b[0;34m(self, module_name)\u001b[0m\n","\u001b[0;32m/usr/lib/python3.12/importlib/__init__.py\u001b[0m in \u001b[0;36mimport_module\u001b[0;34m(name, package)\u001b[0m\n\u001b[1;32m     89\u001b[0m             \u001b[0mlevel\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_bootstrap\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gcd_import\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlevel\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpackage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/models/auto/tokenization_auto.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m \u001b[0;32mif\u001b[0m \u001b[0mis_tokenizers_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0;34m...\u001b[0m\u001b[0mtokenization_utils_fast\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPreTrainedTokenizerFast\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/models/auto/auto_factory.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0mThis\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0ma\u001b[0m \u001b[0mgeneric\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mthat\u001b[0m \u001b[0mwill\u001b[0m \u001b[0mbe\u001b[0m \u001b[0minstantiated\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mone\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0mclasses\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mlibrary\u001b[0m \u001b[0mwhen\u001b[0m \u001b[0mcreated\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mthe\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m~\u001b[0m\u001b[0mBaseAutoModelClass\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mmethod\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mthe\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m~\u001b[0m\u001b[0mBaseAutoModelClass\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_config\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mclass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m     \u001b[0mmethod\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mImportError\u001b[0m: cannot import name 'GenerationMixin' from 'transformers.generation' (/usr/local/lib/python3.12/dist-packages/transformers/generation/__init__.py)","\nDuring handling of the above exception, another exception occurred:\n","\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)","\u001b[0;32m/tmp/ipython-input-3563112187.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"‚ùå –û—à–∏–±–∫–∞ –∏–º–ø–æ—Ä—Ç–∞ transformers: {e}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'pip install transformers==4.35.2'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m     \u001b[0;32mfrom\u001b[0m \u001b[0mtransformers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mAutoTokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAutoModelForTokenClassification\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mget_scheduler\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/utils/import_utils.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/utils/import_utils.py\u001b[0m in \u001b[0;36m_get_module\u001b[0;34m(self, module_name)\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/utils/import_utils.py\u001b[0m in \u001b[0;36m_get_module\u001b[0;34m(self, module_name)\u001b[0m\n","\u001b[0;32m/usr/lib/python3.12/importlib/__init__.py\u001b[0m in \u001b[0;36mimport_module\u001b[0;34m(name, package)\u001b[0m\n\u001b[1;32m     88\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m             \u001b[0mlevel\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_bootstrap\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gcd_import\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlevel\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpackage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/optimization.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlr_scheduler\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mLambdaLR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mReduceLROnPlateau\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mtrainer_utils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSchedulerType\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlogging\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mversions\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mrequire_version\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/trainer_utils.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m from .utils import (\n\u001b[0m\u001b[1;32m     33\u001b[0m     \u001b[0mExplicitEnum\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[0mis_psutil_available\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mImportError\u001b[0m: cannot import name 'is_torch_tpu_available' from 'transformers.utils' (/usr/local/lib/python3.12/dist-packages/transformers/utils/__init__.py)","","\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"],"errorDetails":{"actions":[{"action":"open_url","actionText":"Open Examples","url":"/notebooks/snippets/importing_libraries.ipynb"}]}}]},{"cell_type":"code","source":["# --- –û—Å–Ω–æ–≤–Ω—ã–µ –ø—É—Ç–∏ –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ ---\n","WHERE_DATA = 'cleared_data'\n","BASE_MODEL_NAME = \"bert\"\n","OUT_DIR = f\"OUTPUT/{WHERE_DATA}/{BASE_MODEL_NAME}\"\n","os.makedirs(OUT_DIR, exist_ok=True)\n","FINAL_METRICS_PATH = f\"{OUT_DIR}/final_training_metrics_per_epoch.csv\"\n","MODEL_PATH = f'MODELS/{WHERE_DATA}/{BASE_MODEL_NAME}'\n","os.makedirs(MODEL_PATH, exist_ok=True)\n","DATA_DIR = f'data/{WHERE_DATA}/'\n","PATIENCE = 2\n","SEED = 42\n"],"metadata":{"id":"bO4KcUNr0ASv"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Hugging Face –Ω–∞—Å—Ç—Ä–æ–π–∫–∏\n","HF_TOKEN= getpass.getpass('–í–≤–µ–¥–∏ HFT —Ç–æ–∫–µ–Ω: ')\n","HF_USERNAME = \"alexflex04\"\n","BERT_REPO_NAME = f\"{HF_USERNAME}/NER_{WHERE_DATA}_bert\"\n","\n","setup_hf_login(HF_TOKEN)"],"metadata":{"id":"0KGUxpTbJQSH"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\""],"metadata":{"id":"t_VWEzvBF9nH"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["random.seed(SEED)\n","np.random.seed(SEED)\n","torch.manual_seed(SEED)\n","torch.cuda.manual_seed_all(SEED)\n","torch.backends.cudnn.deterministic = True\n","torch.backends.cudnn.benchmark = False"],"metadata":{"id":"mFL3k8dl0FOh"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","print(f\"–ò—Å–ø–æ–ª—å–∑—É–µ–º–æ–µ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ: {device}\")"],"metadata":{"id":"jgkUd-P9ov1X"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["CONFIG = {\n","    \"model_checkpoint\": \"DeepPavlov/rubert-base-cased\",\n","    \"num_epochs\": 20,\n","    \"batch_size\": 128,\n","    \"learning_rate\": 2e-5,\n","    \"weight_decay\": 0.01,\n","    \"patience\": PATIENCE,\n","    \"label_list\": [\"O\", \"B-TYPE\", \"I-TYPE\", \"B-BRAND\", \"I-BRAND\", \"B-VOLUME\", \"I-VOLUME\", \"B-PERCENT\", \"I-PERCENT\"],\n","    \"id2label\": {i: label for i, label in enumerate([\"O\", \"B-TYPE\", \"I-TYPE\", \"B-BRAND\", \"I-BRAND\", \"B-VOLUME\", \"I-VOLUME\", \"B-PERCENT\", \"I-PERCENT\"])},\n","    \"label2id\": {label: i for i, label in enumerate([\"O\", \"B-TYPE\", \"I-TYPE\", \"B-BRAND\", \"I-BRAND\", \"B-VOLUME\", \"I-VOLUME\", \"B-PERCENT\", \"I-PERCENT\"])},\n","    \"metrics_csv\": f\"{OUT_DIR}/screening_metrics.csv\",\n","    \"submission_input\": f\"{DATA_DIR}/submission.csv\",\n","    \"submission_output\": f\"{OUT_DIR}/submission_response_bert.csv\"\n","}\n"],"metadata":{"id":"j_L-1esl0O58"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö\n","train_split = pd.read_csv(f\"{DATA_DIR}train.csv\")\n","valid_data = pd.read_csv(f\"{DATA_DIR}val.csv\")\n","\n","def parse_row_to_example(row):\n","    try:\n","        ann = ast.literal_eval(row['annotation'])\n","    except Exception:\n","        ann = []\n","    return (row['sample'], {'entities': ann})\n","\n","train_data = [parse_row_to_example(row) for _, row in train_split.iterrows()]\n","valid_data = [parse_row_to_example(row) for _, row in valid_data.iterrows()]\n","\n","tokenizer = AutoTokenizer.from_pretrained(CONFIG[\"model_checkpoint\"])"],"metadata":{"id":"4j7jeRhb0Zfz"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def _normalize_entity_tuple(entity):\n","    if isinstance(entity, dict) and all(k in entity for k in (\"start\", \"end\", \"label\")):\n","        return int(entity[\"start\"]), int(entity[\"end\"]), str(entity[\"label\"])\n","    if isinstance(entity, (list, tuple)) and len(entity) == 3:\n","        a, b, c = entity\n","        if isinstance(a, int) and isinstance(b, int):\n","            return int(a), int(b), str(c)\n","        if isinstance(a, str) and isinstance(b, int) and isinstance(c, int):\n","            return int(b), int(c), str(a)\n","    raise ValueError(f\"Unknown entity format: {entity}\")\n","\n","def spans_to_bio(text, entities, tokenizer):\n","    tokenized = tokenizer(text, return_offsets_mapping=True, truncation=True, max_length=512)\n","    offsets = tokenized[\"offset_mapping\"]\n","    input_ids = tokenized[\"input_ids\"]\n","    labels = [CONFIG[\"label2id\"][\"O\"]] * len(input_ids)\n","\n","    valid_spans = []\n","    for ent in entities:\n","        try:\n","            s, e, l = _normalize_entity_tuple(ent)\n","        except Exception as ex:\n","            continue\n","\n","        if s < 0 or e < 0 or s >= e or e > len(text):\n","            continue\n","\n","        if l == \"O\":\n","            continue\n","\n","        valid_spans.append((s, e, l))\n","\n","    valid_spans.sort(key=lambda x: (x[0], -x[1]))\n","    merged = []\n","    for s, e, l in valid_spans:\n","        if merged and s < merged[-1][1]:\n","            prev_s, prev_e, prev_l = merged[-1]\n","            if (e - s) > (prev_e - prev_s):\n","                merged[-1] = (s, e, l)\n","        else:\n","            merged.append((s, e, l))\n","\n","    for s, e, l in merged:\n","        if l.startswith((\"B-\", \"I-\")):\n","            base = l.split(\"-\", 1)[1]\n","        else:\n","            base = l\n","\n","        b_tag = f\"B-{base}\"\n","        i_tag = f\"I-{base}\"\n","\n","        if b_tag not in CONFIG[\"label2id\"]:\n","            continue\n","\n","        b_id = CONFIG[\"label2id\"][b_tag]\n","        i_id = CONFIG[\"label2id\"][i_tag]\n","\n","        token_indices = []\n","        for idx, (tok_start, tok_end) in enumerate(offsets):\n","            if tok_start == tok_end:\n","                continue\n","            if max(tok_start, s) < min(tok_end, e):\n","                token_indices.append(idx)\n","\n","        if not token_indices:\n","            continue\n","\n","        for i, idx in enumerate(token_indices):\n","            if i == 0:\n","                labels[idx] = b_id\n","            else:\n","                labels[idx] = i_id\n","\n","    return tokenized[\"input_ids\"], tokenized[\"attention_mask\"], labels\n","\n","def bio_to_spans(text, bio_labels, offsets):\n","    entities = []\n","    current_entity = None\n","\n","    for i, (label_id, (start, end)) in enumerate(zip(bio_labels, offsets)):\n","        if start == end:\n","            continue\n","\n","        label = CONFIG[\"id2label\"][int(label_id)]\n","\n","        if label == \"O\":\n","            if current_entity is not None:\n","                entities.append(current_entity)\n","                current_entity = None\n","            continue\n","\n","        if label.startswith(\"B-\"):\n","            if current_entity is not None:\n","                entities.append(current_entity)\n","            entity_type = label[2:]\n","            current_entity = (start, end, entity_type)\n","\n","        elif label.startswith(\"I-\"):\n","            entity_type = label[2:]\n","            if current_entity is not None and current_entity[2] == entity_type:\n","                current_entity = (current_entity[0], end, entity_type)\n","            else:\n","                if current_entity is not None:\n","                    entities.append(current_entity)\n","                current_entity = (start, end, entity_type)\n","\n","    if current_entity is not None:\n","        entities.append(current_entity)\n","\n","    return entities"],"metadata":{"id":"ge5Fxkel0aFT"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(\"\\n=== –ü–†–û–í–ï–†–ö–ê spans_to_bio / bio_to_spans ===\")\n","sample_text = \"–º–æ–ª–æ–∫–æ –ü—Ä–æ—Å—Ç–æ–∫–≤–∞—à–∏–Ω–æ 2.5% 1 –ª\"\n","test_entities = [\n","    (0, 6, \"TYPE\"),\n","    {\"start\": 7, \"end\": 19, \"label\": \"BRAND\"},\n","    (21, 25, \"PERCENT\"),\n","    (26, 29, \"VOLUME\"),\n","    (30, 10, \"TYPE\"),        # –Ω–µ–∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã–π\n","    (0, 1000, \"TYPE\")        # –≤–Ω–µ –≥—Ä–∞–Ω–∏—Ü\n","]\n","ids, mask, labs = spans_to_bio(sample_text, test_entities, tokenizer)\n","print(\"–¢–æ–∫–µ–Ω—ã:\", tokenizer.convert_ids_to_tokens(ids))\n","print(\"BIO:\", [CONFIG[\"id2label\"][x] for x in labs])\n","offs = tokenizer(sample_text, return_offsets_mapping=True)[\"offset_mapping\"]\n","print(\"–ù–∞–∑–∞–¥ –≤ —Å–ø–∞–Ω—ã:\", bio_to_spans(sample_text, labs, offs))\n","\n","# –ü–µ—Ä–µ—Å–µ—á–µ–Ω–∏—è\n","print(\"\\n=== –ü—Ä–æ–≤–µ—Ä–∫–∞ –ø–µ—Ä–µ—Å–µ–∫–∞—é—â–∏—Ö—Å—è —Å–ø–∞–Ω–æ–≤ ===\")\n","text2 = \"—á–∏–ø—Å—ã –ª–µ–π—Å –∫—Ä–∞–±\"\n","ents2 = [(0, 5, \"TYPE\"), (4, 9, \"BRAND\")]\n","_, _, labs2 = spans_to_bio(text2, ents2, tokenizer)\n","print(\"BIO:\", [CONFIG[\"id2label\"][x] for x in labs2])\n","print(\"–ù–∞–∑–∞–¥ –≤ —Å–ø–∞–Ω—ã:\", bio_to_spans(text2, labs2, tokenizer(text2, return_offsets_mapping=True)[\"offset_mapping\"]))\n"],"metadata":{"id":"o-hGbaZA0cMk"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class NERDataset(Dataset):\n","    def __init__(self, data, tokenizer):\n","        self.data = data\n","        self.tokenizer = tokenizer\n","        self.processed_data = []\n","\n","        for text, ann in data:\n","            try:\n","                input_ids, attention_mask, labels = spans_to_bio(text, ann['entities'], tokenizer)\n","                self.processed_data.append({\n","                    \"input_ids\": torch.tensor(input_ids, dtype=torch.long),\n","                    \"attention_mask\": torch.tensor(attention_mask, dtype=torch.long),\n","                    \"labels\": torch.tensor(labels, dtype=torch.long)\n","                })\n","            except Exception as e:\n","                continue\n","\n","    def __len__(self):\n","        return len(self.processed_data)\n","\n","    def __getitem__(self, idx):\n","        return self.processed_data[idx]\n","\n","def collate_fn(batch):\n","    input_ids = [item[\"input_ids\"] for item in batch]\n","    attention_mask = [item[\"attention_mask\"] for item in batch]\n","    labels = [item[\"labels\"] for item in batch]\n","\n","    input_ids = pad_sequence(input_ids, batch_first=True, padding_value=0)\n","    attention_mask = pad_sequence(attention_mask, batch_first=True, padding_value=0)\n","    labels = pad_sequence(labels, batch_first=True, padding_value=0)\n","\n","    return {\n","        \"input_ids\": input_ids,\n","        \"attention_mask\": attention_mask,\n","        \"labels\": labels\n","    }\n","\n","class NERModelWithCRF(torch.nn.Module):\n","    def __init__(self, num_labels):\n","        super().__init__()\n","        self.bert = AutoModelForTokenClassification.from_pretrained(CONFIG[\"model_checkpoint\"], num_labels=num_labels)\n","        self.crf = CRF(num_labels)\n","\n","    def forward(self, input_ids, attention_mask, labels=None):\n","        outputs = self.bert(input_ids, attention_mask=attention_mask)\n","        emissions = outputs.logits\n","        if labels is not None:\n","            loss = -self.crf(emissions, labels, mask=attention_mask.type(torch.uint8))\n","            return loss\n","        else:\n","            return self.crf.decode(emissions, mask=attention_mask.type(torch.uint8))\n","\n","def evaluate_model(model, eval_data, tokenizer):\n","    model.eval()\n","    entity_pairs = []\n","    for text, annotations in eval_data:\n","        tokenized = tokenizer([text], padding=True, truncation=True, return_tensors=\"pt\", return_offsets_mapping=True)\n","        input_ids = tokenized[\"input_ids\"].to(device)\n","        attention_mask = tokenized[\"attention_mask\"].to(device)\n","        with torch.no_grad():\n","            pred = model(input_ids, attention_mask)[0]\n","        offsets = tokenized[\"offset_mapping\"][0].tolist()\n","        pred_spans = bio_to_spans(text, pred, offsets)\n","        true_entities = annotations['entities']\n","        entity_pairs.append((true_entities, pred_spans))\n","\n","    macro_f1, f1_type, f1_brand, f1_volume, f1_percent = calculate_macro_f1(entity_pairs)\n","    return {\n","        'f1_macro': macro_f1,\n","        'f1_TYPE': f1_type,\n","        'f1_BRAND': f1_brand,\n","        'f1_VOLUME': f1_volume,\n","        'f1_PERCENT': f1_percent\n","    }\n"],"metadata":{"id":"ViwlaKNrFIkC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"ueU3QeiOcsC8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(\"=== –ò–ù–ò–¶–ò–ê–õ–ò–ó–ê–¶–ò–Ø –ú–û–î–ï–õ–ò ===\")\n","model = NERModelWithCRF(len(CONFIG[\"label_list\"])).to(device)\n","optimizer = AdamW(model.parameters(), lr=CONFIG[\"learning_rate\"], weight_decay=CONFIG[\"weight_decay\"])\n","num_training_steps = CONFIG[\"num_epochs\"] * len(train_data) // CONFIG[\"batch_size\"]\n","scheduler = get_scheduler(\"linear\", optimizer=optimizer, num_warmup_steps=0, num_training_steps=num_training_steps)\n","\n","train_dataset = NERDataset(train_data, tokenizer)\n","train_loader = DataLoader(train_dataset, batch_size=CONFIG[\"batch_size\"], shuffle=True, collate_fn=collate_fn)\n","\n","valid_dataset = NERDataset(valid_data, tokenizer)\n","valid_loader = DataLoader(valid_dataset, batch_size=CONFIG[\"batch_size\"], shuffle=False, collate_fn=collate_fn)\n","\n","metrics_df = pd.DataFrame(columns=['epoch', 'loss', 'f1_macro', 'f1_TYPE', 'f1_BRAND', 'f1_VOLUME', 'f1_PERCENT'])\n","best_f1 = 0\n","patience_counter = 0\n","best_epoch = 0\n","\n","print(\"‚úÖ –ú–æ–¥–µ–ª—å –∏ –¥–∞–Ω–Ω—ã–µ —É—Å–ø–µ—à–Ω–æ –ø–æ–¥–≥–æ—Ç–æ–≤–ª–µ–Ω—ã!\")\n","print(f\"–†–∞–∑–º–µ—Ä –æ–±—É—á–∞—é—â–µ–π –≤—ã–±–æ—Ä–∫–∏: {len(train_dataset)}\")\n","print(f\"–†–∞–∑–º–µ—Ä –≤–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–Ω–æ–π –≤—ã–±–æ—Ä–∫–∏: {len(valid_dataset)}\")"],"metadata":{"id":"92aearUZ0hEQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(\"\\n=== –ù–ê–ß–ê–õ–û SCREENING –û–ë–£–ß–ï–ù–ò–Ø ===\")\n","try:\n","    for epoch in range(CONFIG[\"num_epochs\"]):\n","        model.train()\n","        total_loss = 0\n","        for batch in train_loader:\n","            input_ids = batch[\"input_ids\"].to(device)\n","            attention_mask = batch[\"attention_mask\"].to(device)\n","            labels = batch[\"labels\"].to(device)\n","            loss = model(input_ids, attention_mask, labels)\n","            total_loss += loss.item()\n","            optimizer.zero_grad()\n","            loss.backward()\n","            optimizer.step()\n","            scheduler.step()\n","\n","        avg_loss = total_loss / len(train_loader)\n","        eval_metrics = evaluate_model(model, valid_data, tokenizer)\n","        current_f1 = eval_metrics[\"f1_macro\"]\n","\n","        metrics_row = {\n","            'epoch': epoch + 1,\n","            'loss': avg_loss,\n","            **eval_metrics\n","        }\n","        metrics_df = pd.concat([metrics_df, pd.DataFrame([metrics_row])], ignore_index=True)\n","\n","        print(f'–≠–ø–æ—Ö–∞ {epoch + 1:<3} | Loss: {avg_loss:.4f} | '\n","              f'F1-macro: {current_f1:.4f} | '\n","              f'F1-TYPE: {eval_metrics[\"f1_TYPE\"]:.4f} | '\n","              f'F1-BRAND: {eval_metrics[\"f1_BRAND\"]:.4f} | '\n","              f'F1-VOLUME: {eval_metrics[\"f1_VOLUME\"]:.4f} | '\n","              f'F1-PERCENT: {eval_metrics[\"f1_PERCENT\"]:.4f}')\n","\n","        if current_f1 > best_f1:\n","            best_f1 = current_f1\n","            best_epoch = epoch + 1\n","            patience_counter = 0\n","        else:\n","            patience_counter += 1\n","            print(f\"‚è≥ Patience: {patience_counter}/{PATIENCE}\")\n","            if patience_counter >= PATIENCE:\n","                print(f\"\\nüõë –†–∞–Ω–Ω—è—è –æ—Å—Ç–∞–Ω–æ–≤–∫–∞ –Ω–∞ —ç–ø–æ—Ö–µ {epoch + 1}\")\n","                print(f\"–õ—É—á—à–∏–π F1-macro: {best_f1:.4f} –¥–æ—Å—Ç–∏–≥–Ω—É—Ç –Ω–∞ —ç–ø–æ—Ö–µ {best_epoch}\")\n","                break\n","\n","except Exception as e:\n","    print(f'üí• –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞: {str(e)}')\n","    print(traceback.format_exc())\n","\n","finally:\n","    # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ screening –º–æ–¥–µ–ª–∏ –Ω–∞ HF\n","    print(f\"\\nüíæ –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ screening –º–æ–¥–µ–ª–∏ –Ω–∞ HF: {BERT_REPO_NAME+'_screening'}\")\n","    success = save_bert_to_hf(model, tokenizer, CONFIG, BERT_REPO_NAME+'_screening', HF_TOKEN)\n","\n","    if success:\n","        print(f\"üéâ BERT screening –º–æ–¥–µ–ª—å —É—Å–ø–µ—à–Ω–æ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –Ω–∞ HF: {BERT_REPO_NAME+'_screening'}\")\n","    else:\n","        print(\"‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å —Å–æ—Ö—Ä–∞–Ω–∏—Ç—å BERT screening –º–æ–¥–µ–ª—å –Ω–∞ HF\")\n","\n","    # –õ–æ–∫–∞–ª—å–Ω–æ–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ\n","    # torch.save(model.state_dict(), f\"{MODEL_PATH}/model_screening.pt\")\n","    metrics_df.to_csv(CONFIG[\"metrics_csv\"], index=False)\n","    print(\"üíæ Screening –º–æ–¥–µ–ª—å –∏ –º–µ—Ç—Ä–∏–∫–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –ª–æ–∫–∞–ª—å–Ω–æ\")\n","\n","print(\"\\n\" + \"=\"*80)\n","print(\"–ò–¢–û–ì–û–í–´–ï –†–ï–ó–£–õ–¨–¢–ê–¢–´ SCREENING:\")\n","print(\"=\"*80)\n","print(f\"–õ—É—á—à–∏–π F1-macro: {best_f1:.4f} –Ω–∞ —ç–ø–æ—Ö–µ {best_epoch}\")\n","print(f\"–í—Å–µ–≥–æ —ç–ø–æ—Ö –≤—ã–ø–æ–ª–Ω–µ–Ω–æ: {len(metrics_df)}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":998},"id":"8IyWSwnO0jQ4","executionInfo":{"status":"error","timestamp":1759144612773,"user_tz":-180,"elapsed":709,"user":{"displayName":"TVLG LX6500","userId":"15785277625807876484"}},"outputId":"614cdb4a-cfec-42e1-be32-9cb70375d35a"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["üí• –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞: CUDA error: device-side assert triggered\n","Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n","\n","Traceback (most recent call last):\n","  File \"/tmp/ipython-input-668927516.py\", line 9, in <cell line: 0>\n","    loss = model(input_ids, attention_mask, labels)\n","           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n","  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1773, in _wrapped_call_impl\n","    return self._call_impl(*args, **kwargs)\n","           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n","  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1784, in _call_impl\n","    return forward_call(*args, **kwargs)\n","           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n","  File \"/tmp/ipython-input-3522910203.py\", line 48, in forward\n","    loss = -self.crf(emissions, labels, mask=attention_mask.type(torch.uint8))\n","            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n","  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1773, in _wrapped_call_impl\n","    return self._call_impl(*args, **kwargs)\n","           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n","  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1784, in _call_impl\n","    return forward_call(*args, **kwargs)\n","           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n","  File \"/usr/local/lib/python3.12/dist-packages/TorchCRF/__init__.py\", line 49, in forward\n","    log_numerator = self._compute_numerator_log_likelihood(h, labels, mask)\n","                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n","  File \"/usr/local/lib/python3.12/dist-packages/TorchCRF/__init__.py\", line 204, in _compute_numerator_log_likelihood\n","    [self._calc_trans_score_for_num_llh(\n","     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n","  File \"/usr/local/lib/python3.12/dist-packages/TorchCRF/__init__.py\", line 253, in _calc_trans_score_for_num_llh\n","    trans_t = trans[y[:, t], y[:, t + 1]].squeeze(1)\n","              ~~~~~^^^^^^^^^^^^^^^^^^^^^^\n","torch.AcceleratorError: CUDA error: device-side assert triggered\n","Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n","\n","\n","‚ö†Ô∏è –û–±—É—á–µ–Ω–∏–µ –ø—Ä–µ—Ä–≤–∞–Ω–æ –∏–∑-–∑–∞ –æ—à–∏–±–∫–∏, —Å–æ—Ö—Ä–∞–Ω–µ–Ω —Ç–µ–∫—É—â–∏–π –ø—Ä–æ–≥—Ä–µ—Å—Å\n"]},{"output_type":"error","ename":"AcceleratorError","evalue":"CUDA error: device-side assert triggered\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAcceleratorError\u001b[0m                          Traceback (most recent call last)","\u001b[0;32m/tmp/ipython-input-668927516.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0;31m# –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ñ–∏–Ω–∞–ª—å–Ω–æ–π –º–æ–¥–µ–ª–∏ –ø–æ—Å–ª–µ —Å–∫—Ä–∏–Ω–∏–Ω–≥–∞\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     \u001b[0;31m# –í–∞—Ä–∏–∞–Ω—Ç 1: –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –ø–æ–ª–Ω–æ–π –º–æ–¥–µ–ª–∏ –∏ BERT\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m     \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mf\"{MODEL_PATH}/model_screening.pt\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     61\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbert\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{MODEL_PATH}_screening_final\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;31m# –í–∞—Ä–∏–∞–Ω—Ç 2: –≠–∫—Å–ø–æ—Ä—Ç –≤ ONNX (–∑–∞–∫–æ–º–º–µ–Ω—Ç–∏—Ä–æ–≤–∞–Ω)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(obj, f, pickle_module, pickle_protocol, _use_new_zipfile_serialization, _disable_byteorder_record)\u001b[0m\n\u001b[1;32m    965\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0m_use_new_zipfile_serialization\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    966\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0m_open_zipfile_writer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mopened_zipfile\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 967\u001b[0;31m             _save(\n\u001b[0m\u001b[1;32m    968\u001b[0m                 \u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    969\u001b[0m                 \u001b[0mopened_zipfile\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_save\u001b[0;34m(obj, zip_file, pickle_module, pickle_protocol, _disable_byteorder_record)\u001b[0m\n\u001b[1;32m   1264\u001b[0m                     \u001b[0mstorage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_storage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1265\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1266\u001b[0;31m                     \u001b[0mstorage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstorage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1267\u001b[0m             \u001b[0;31m# Now that it is on the CPU we can directly copy it into the zip file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1268\u001b[0m             \u001b[0mzip_file\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite_record\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstorage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_bytes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/storage.py\u001b[0m in \u001b[0;36mcpu\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    264\u001b[0m         \u001b[0;34m\"\"\"Return a CPU copy of this storage if it's not already on the CPU.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    265\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m\"cpu\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 266\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mUntypedStorage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    267\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    268\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mAcceleratorError\u001b[0m: CUDA error: device-side assert triggered\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}]},{"cell_type":"code","source":["# –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è\n","plt.figure(figsize=(15, 10))\n","plt.subplot(2, 1, 1)\n","plt.plot(metrics_df['epoch'], metrics_df['loss'], 'b-', linewidth=2, label='Loss')\n","plt.axvline(x=best_epoch, color='r', linestyle='--', alpha=0.7, label=f'–õ—É—á—à–∞—è —ç–ø–æ—Ö–∞ ({best_epoch})')\n","plt.xlabel('–≠–ø–æ—Ö–∞')\n","plt.ylabel('Loss')\n","plt.title('Loss –ø–æ —ç–ø–æ—Ö–∞–º')\n","plt.legend()\n","plt.grid(True, alpha=0.3)\n","\n","plt.subplot(2, 1, 2)\n","plt.plot(metrics_df['epoch'], metrics_df['f1_macro'], 'r-', linewidth=3, label='F1-macro')\n","plt.plot(metrics_df['epoch'], metrics_df['f1_TYPE'], 'g--', label='F1-TYPE')\n","plt.plot(metrics_df['epoch'], metrics_df['f1_BRAND'], 'b--', label='F1-BRAND')\n","plt.plot(metrics_df['epoch'], metrics_df['f1_VOLUME'], 'y--', label='F1-VOLUME')\n","plt.plot(metrics_df['epoch'], metrics_df['f1_PERCENT'], 'c--', label='F1-PERCENT')\n","plt.axvline(x=best_epoch, color='r', linestyle='--', alpha=0.7, label=f'–õ—É—á—à–∞—è —ç–ø–æ—Ö–∞ ({best_epoch})')\n","plt.xlabel('–≠–ø–æ—Ö–∞')\n","plt.ylabel('F1 Score')\n","plt.title('F1 Scores –ø–æ —ç–ø–æ—Ö–∞–º')\n","plt.legend()\n","plt.grid(True, alpha=0.3)\n","\n","plt.tight_layout()\n","plt.savefig(f\"{OUT_DIR}/screening_metrics.png\", dpi=300, bbox_inches='tight')\n","plt.show()"],"metadata":{"id":"a8NbgDCk0mEW"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(\"\\n=== –ü–†–û–í–ï–†–ö–ê –ó–ê–ì–†–£–ó–ö–ò –ú–û–î–ï–õ–ò ===\")\n","loaded_model, loaded_tokenizer, loaded_config = load_bert_from_hf(BERT_REPO_NAME+'_screening', HF_TOKEN, device)\n","\n","if loaded_model:\n","    print(\"‚úÖ –ú–æ–¥–µ–ª—å —É—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω–∞ —Å HF!\")\n","    test_text = \"–º–æ–ª–æ–∫–æ –ü—Ä–æ—Å—Ç–æ–∫–≤–∞—à–∏–Ω–æ 2.5% 1–ª\"\n","    from module import HFWrapper\n","    wrapper = HFWrapper(loaded_model, loaded_tokenizer)\n","    doc = wrapper(test_text)\n","    entities = [(ent.start_char, ent.end_char, ent.label_) for ent in doc.ents]\n","    print(f\"–¢–µ—Å—Ç–æ–≤—ã–π —Ç–µ–∫—Å—Ç: '{test_text}'\")\n","    print(f\"–ò–∑–≤–ª–µ—á–µ–Ω–Ω—ã–µ —Å—É—â–Ω–æ—Å—Ç–∏: {entities}\")\n","\n","    # –û–±—Ä–∞–±–æ—Ç–∫–∞ submission —Ñ–∞–π–ª–∞\n","    print(f\"\\n=== –û–ë–†–ê–ë–û–¢–ö–ê SUBMISSION –§–ê–ô–õ–ê ===\")\n","    process_submission_bert(\n","        model=loaded_model,\n","        tokenizer=loaded_tokenizer,\n","        input_file=CONFIG[\"submission_input\"],\n","        output_file=f\"{OUT_DIR}/submission_screening.csv\"\n","    )\n","else:\n","    print(\"‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å –º–æ–¥–µ–ª—å –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è\")"],"metadata":{"id":"oVXcgPQDKOoV"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"cM7CtsGwKO5w"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# –Ø—á–µ–π–∫–∞ 2: –ü–æ–¥–±–æ—Ä –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ (Tuning) —Å grid search\n","PARAM_GRID = {\n","    \"learning_rate\": [1e-5, 2e-5, 3e-5],\n","    \"batch_size\": [32, 64],\n","    \"epochs\": [10, 20],\n","    \"weight_decay\": [0.01, 0.1]\n","}\n","\n","grid_results = []\n","\n","for lr in PARAM_GRID[\"learning_rate\"]:\n","    for bsz in PARAM_GRID[\"batch_size\"]:\n","        for max_ep in PARAM_GRID[\"epochs\"]:\n","            for wd in PARAM_GRID[\"weight_decay\"]:\n","                combo = {\"learning_rate\": lr, \"batch_size\": bsz, \"epochs\": max_ep, \"weight_decay\": wd}\n","                print(f\"\\n=== Tuning combo: learning_rate={lr}, batch_size={bsz}, epochs={max_ep}, weight_decay={wd} ===\")\n","\n","                model = NERModelWithCRF(len(CONFIG[\"label_list\"])).to(device)\n","                optimizer = AdamW(model.parameters(), lr=lr, weight_decay=wd)\n","                num_training_steps = max_ep * len(train_data) // bsz\n","                scheduler = get_scheduler(\"linear\", optimizer=optimizer, num_warmup_steps=0, num_training_steps=num_training_steps)\n","\n","                train_loader = DataLoader(train_dataset, batch_size=bsz, shuffle=True)\n","\n","                patience_counter, best_f1, best_metrics = 0, 0.0, None\n","                for epoch in range(1, max_ep + 1):\n","                    model.train()\n","                    total_loss = 0\n","                    for batch in train_loader:\n","                        input_ids = batch[\"input_ids\"].to(device)\n","                        attention_mask = batch[\"attention_mask\"].to(device)\n","                        labels = batch[\"labels\"].to(device)\n","                        loss = model(input_ids, attention_mask, labels)\n","                        total_loss += loss.item()\n","                        optimizer.zero_grad()\n","                        loss.backward()\n","                        optimizer.step()\n","                        scheduler.step()\n","\n","                    avg_loss = total_loss / len(train_loader)\n","                    metrics = evaluate_model(model, valid_data, tokenizer)\n","                    metrics[\"epoch\"] = epoch\n","                    metrics[\"loss\"] = avg_loss\n","                    current_f1 = metrics[\"f1_macro\"]\n","\n","                    print(f\"Ep {epoch} | Loss: {metrics['loss']:.4f} | F1-macro: {current_f1:.4f}\")\n","\n","                    if current_f1 > best_f1:\n","                        best_f1 = current_f1\n","                        best_metrics = metrics\n","                        patience_counter = 0\n","                    else:\n","                        patience_counter += 1\n","                        if patience_counter >= PATIENCE:\n","                            break\n","\n","                combo[\"best_f1_macro\"] = best_f1\n","                combo[\"best_metrics\"] = best_metrics\n","                grid_results.append(combo)\n","\n","# –í—ã–±–æ—Ä –ª—É—á—à–∏—Ö –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤\n","best_combo = max(grid_results, key=lambda x: x[\"best_f1_macro\"])\n","print(\"\\nBest tuning params:\", best_combo)\n","\n","# –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤\n","pd.DataFrame(grid_results).to_csv(f\"{OUT_DIR}/tuning_summary.csv\", index=False)\n","with open(f\"{OUT_DIR}/tuning_detailed.json\", \"w\", encoding=\"utf-8\") as f:\n","    json.dump(grid_results, f, ensure_ascii=False, indent=2)\n","with open(f\"{OUT_DIR}/best_combo.json\", \"w\", encoding=\"utf-8\") as f:\n","    json.dump({k: v for k, v in best_combo.items() if k != \"best_metrics\"}, f, ensure_ascii=False, indent=2)\n","print(\"üíæ Tuning —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –∏ best_combo —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã\")\n"],"metadata":{"id":"6vKo5oFn0s2V"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# –Ø—á–µ–π–∫–∞ 3: –ö—Ä–æ—Å—Å-–≤–∞–ª–∏–¥–∞—Ü–∏—è (CV) —Å –ª—É—á—à–∏–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏\n","with open(f\"{OUT_DIR}/best_combo.json\", \"r\", encoding=\"utf-8\") as f:\n","    best_combo = json.load(f)\n","\n","best_lr = best_combo[\"learning_rate\"]\n","best_bsz = best_combo[\"batch_size\"]\n","best_max_ep = best_combo[\"epochs\"]\n","best_wd = best_combo[\"weight_decay\"]\n","\n","kf = KFold(n_splits=5, shuffle=True, random_state=SEED)\n","cv_results = []\n","fold_best_f1s = []\n","\n","for fold, (tr_idx, val_idx) in enumerate(kf.split(train_data), 1):\n","    print(f\"\\n=== CV Fold {fold} ===\")\n","    fold_train = [train_data[i] for i in tr_idx]\n","    fold_valid = [train_data[i] for i in val_idx]\n","\n","    fold_train_dataset = NERDataset(fold_train, tokenizer)\n","    fold_train_loader = DataLoader(fold_train_dataset, batch_size=best_bsz, shuffle=True)\n","\n","    model = NERModelWithCRF(len(CONFIG[\"label_list\"])).to(device)\n","    optimizer = AdamW(model.parameters(), lr=best_lr, weight_decay=best_wd)\n","    num_training_steps = best_max_ep * len(fold_train) // best_bsz\n","    scheduler = get_scheduler(\"linear\", optimizer=optimizer, num_warmup_steps=0, num_training_steps=num_training_steps)\n","\n","    patience_counter, best_f1, best_metrics = 0, 0.0, None\n","    for epoch in range(1, best_max_ep + 1):\n","        model.train()\n","        total_loss = 0\n","        for batch in fold_train_loader:\n","            input_ids = batch[\"input_ids\"].to(device)\n","            attention_mask = batch[\"attention_mask\"].to(device)\n","            labels = batch[\"labels\"].to(device)\n","            loss = model(input_ids, attention_mask, labels)\n","            total_loss += loss.item()\n","            optimizer.zero_grad()\n","            loss.backward()\n","            optimizer.step()\n","            scheduler.step()\n","\n","        avg_loss = total_loss / len(fold_train_loader)\n","        metrics = evaluate_model(model, fold_valid, tokenizer)\n","        metrics[\"epoch\"] = epoch\n","        metrics[\"loss\"] = avg_loss\n","        current_f1 = metrics[\"f1_macro\"]\n","\n","        print(f\"Fold {fold} Ep {epoch} | Loss: {metrics['loss']:.4f} | F1-macro: {current_f1:.4f}\")\n","\n","        if current_f1 > best_f1:\n","            best_f1 = current_f1\n","            best_metrics = metrics\n","            patience_counter = 0\n","        else:\n","            patience_counter += 1\n","            if patience_counter >= PATIENCE:\n","                break\n","\n","    cv_results.append({\"fold\": fold, \"best_f1_macro\": best_f1, \"best_metrics\": best_metrics})\n","    fold_best_f1s.append(best_f1)\n","\n","mean_f1 = np.mean(fold_best_f1s)\n","std_f1 = np.std(fold_best_f1s)\n","print(f\"\\nCV Results: Mean F1_macro = {mean_f1:.4f} ¬± {std_f1:.4f}\")\n","\n","# –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ CV\n","pd.DataFrame(cv_results).to_csv(f\"{OUT_DIR}/cv_summary.csv\", index=False)\n","with open(f\"{OUT_DIR}/cv_detailed.json\", \"w\", encoding=\"utf-8\") as f:\n","    json.dump(cv_results, f, ensure_ascii=False, indent=2)\n","print(\"üíæ CV —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã\")"],"metadata":{"id":"gU0AhvFT0tZq"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# –Ø—á–µ–π–∫–∞ 4: –§–∏–Ω–∞–ª—å–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ –Ω–∞ –æ–±—ä–µ–¥–∏–Ω—ë–Ω–Ω–æ–º –¥–∞—Ç–∞—Å–µ—Ç–µ (train+val)\n","train_val = [(row['sample'], {'entities': ast.literal_eval(row['annotation'])}) for _, row in pd.concat([train_split, valid_data]).iterrows()]\n","train_val_dataset = NERDataset(train_val, tokenizer)\n","train_val_loader = DataLoader(train_val_dataset, batch_size=best_bsz, shuffle=True)\n","\n","model = NERModelWithCRF(len(CONFIG[\"label_list\"])).to(device)\n","optimizer = AdamW(model.parameters(), lr=best_lr, weight_decay=best_wd)\n","num_training_steps = best_max_ep * len(train_val) // best_bsz\n","scheduler = get_scheduler(\"linear\", optimizer=optimizer, num_warmup_steps=0, num_training_steps=num_training_steps)\n","\n","records = []\n","best_final_f1, patience_counter = 0.0, 0\n","for epoch in range(1, best_max_ep + 1):\n","    model.train()\n","    total_loss = 0\n","    for batch in train_val_loader:\n","        input_ids = batch[\"input_ids\"].to(device)\n","        attention_mask = batch[\"attention_mask\"].to(device)\n","        labels = batch[\"labels\"].to(device)\n","        loss = model(input_ids, attention_mask, labels)\n","        total_loss += loss.item()\n","        optimizer.zero_grad()\n","        loss.backward()\n","        optimizer.step()\n","        scheduler.step()\n","\n","    avg_loss = total_loss / len(train_val_loader)\n","    print(f\"–≠–ø–æ—Ö–∞ {epoch} | Loss: {avg_loss:.4f}\")\n","\n","# –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ screening –º–æ–¥–µ–ª–∏ –Ω–∞ HF\n","    print(f\"\\nüíæ –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ screening –º–æ–¥–µ–ª–∏ –Ω–∞ HF: {BERT_REPO_NAME}\")\n","    success = save_bert_to_hf(model, tokenizer, CONFIG, BERT_REPO_NAME, HF_TOKEN)\n","\n","    if success:\n","        print(f\"üéâ BERT screening –º–æ–¥–µ–ª—å —É—Å–ø–µ—à–Ω–æ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –Ω–∞ HF: {BERT_REPO_NAME}\")\n","    else:\n","        print(\"‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å —Å–æ—Ö—Ä–∞–Ω–∏—Ç—å BERT screening –º–æ–¥–µ–ª—å –Ω–∞ HF\")\n","# –í–∞—Ä–∏–∞–Ω—Ç 1: –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ BERT-–º–æ–¥–µ–ª–∏\n","# model.bert.save_pretrained(MODEL_PATH)\n","# –í–∞—Ä–∏–∞–Ω—Ç 2: –≠–∫—Å–ø–æ—Ä—Ç –≤ ONNX (–∑–∞–∫–æ–º–º–µ–Ω—Ç–∏—Ä–æ–≤–∞–Ω)\n","# dummy_input_ids = torch.randint(0, tokenizer.vocab_size, (1, 512)).to(device)\n","# dummy_attention_mask = torch.ones(1, 512).to(device)\n","# torch.onnx.export(model.bert, (dummy_input_ids, dummy_attention_mask),\n","#                   CONFIG[\"onnx_model_path\"],\n","#                   export_params=True,\n","#                   opset_version=14,  # –ò–∑–º–µ–Ω–µ–Ω–æ –Ω–∞ 14\n","#                   input_names=['input_ids', 'attention_mask'],\n","#                   output_names=['logits'],\n","#                   dynamic_axes={'input_ids': {0: 'batch', 1: 'seq'},\n","#                                 'attention_mask': {0: 'batch', 1: 'seq'},\n","#                                 'logits': {0: 'batch', 1: 'seq'}})\n","# quantize_dynamic(CONFIG[\"onnx_model_path\"], CONFIG[\"quantized_onnx_path\"], weight_type=QuantType.QUInt8)\n","# print(f\"\\nFinal model saved: {MODEL_PATH}\")"],"metadata":{"id":"D7b_w0Nd0v2i"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"Wz7v_fJW0ycg"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(\"\\n=== –ü–†–û–í–ï–†–ö–ê –ó–ê–ì–†–£–ó–ö–ò –ú–û–î–ï–õ–ò ===\")\n","loaded_model, loaded_tokenizer, loaded_config = load_bert_from_hf(BERT_REPO_NAME, HF_TOKEN, device)\n","\n","if loaded_model:\n","    print(\"‚úÖ –ú–æ–¥–µ–ª—å —É—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω–∞ —Å HF!\")\n","    test_text = \"–º–æ–ª–æ–∫–æ –ü—Ä–æ—Å—Ç–æ–∫–≤–∞—à–∏–Ω–æ 2.5% 1–ª\"\n","    from module import HFWrapper\n","    wrapper = HFWrapper(loaded_model, loaded_tokenizer)\n","    doc = wrapper(test_text)\n","    entities = [(ent.start_char, ent.end_char, ent.label_) for ent in doc.ents]\n","    print(f\"–¢–µ—Å—Ç–æ–≤—ã–π —Ç–µ–∫—Å—Ç: '{test_text}'\")\n","    print(f\"–ò–∑–≤–ª–µ—á–µ–Ω–Ω—ã–µ —Å—É—â–Ω–æ—Å—Ç–∏: {entities}\")\n","\n","    # –û–±—Ä–∞–±–æ—Ç–∫–∞ submission —Ñ–∞–π–ª–∞\n","    print(f\"\\n=== –û–ë–†–ê–ë–û–¢–ö–ê SUBMISSION –§–ê–ô–õ–ê ===\")\n","    process_submission_bert(\n","        model=loaded_model,\n","        tokenizer=loaded_tokenizer,\n","        input_file=CONFIG[\"submission_input\"],\n","        output_file=f\"{OUT_DIR}/submission.csv\"\n","    )\n","else:\n","    print(\"‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å –º–æ–¥–µ–ª—å –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è\")"],"metadata":{"id":"LZfZkz6oL3-x"},"execution_count":null,"outputs":[]}]}