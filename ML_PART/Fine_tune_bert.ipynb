{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":58256,"status":"ok","timestamp":1759343738916,"user":{"displayName":"TVLG LX6500","userId":"15785277625807876484"},"user_tz":-180},"id":"0jU5kuXfijpT","outputId":"cc73f42a-0218-4442-9536-344fe2366134"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n","/content/drive/MyDrive/Colab Notebooks\n","–í–≤–µ–¥–∏ GitHub PAT —Ç–æ–∫–µ–Ω: ¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑\n","–ó–∞–Ω–æ–≤–æ —Å–∫–ª–æ–Ω–∏—Ä–æ–≤–∞–ª–∏ —Ä–µ–ø—É\n","fatal: destination path 'entities-extraction-x5' already exists and is not an empty directory.\n","/content/drive/MyDrive/Colab Notebooks/entities-extraction-x5/ML_PART\n","‚úÖ –í—Å—ë –≥–æ—Ç–æ–≤–æ! –†–∞–±–æ—á–∞—è –ø–∞–ø–∫–∞: /content/drive/MyDrive/Colab Notebooks/entities-extraction-x5/ML_PART\n"]}],"source":["from google.colab import drive\n","import getpass, os\n","\n","# === –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ø—Ä–æ–µ–∫—Ç–∞ ===\n","USER = \"tokarevdr\"   # —Ç–≤–æ–π GitHub username\n","REPO = \"entities-extraction-x5\"            # –Ω–∞–∑–≤–∞–Ω–∏–µ —Ä–µ–ø–æ–∑–∏—Ç–æ—Ä–∏—è\n","EMAIL = \"fedorov.alexander.04@gmail.com\"    # —Ç–≤–æ—è –ø–æ—á—Ç–∞ –¥–ª—è git\n","NAME = \"Alexander\"           # —Ç–≤–æ—ë –∏–º—è –¥–ª—è git\n","# === –ü–æ–¥–∫–ª—é—á–µ–Ω–∏–µ Google Drive ===\n","drive.mount('/content/drive')\n","PROJECTS_DIR = \"/content/drive/MyDrive/Colab Notebooks\"\n","%cd $PROJECTS_DIR\n","# === GitHub –∞–≤—Ç–æ—Ä–∏–∑–∞—Ü–∏—è ===\n","token = getpass.getpass('–í–≤–µ–¥–∏ GitHub PAT —Ç–æ–∫–µ–Ω: ')\n","os.environ[\"GITHUB_TOKEN\"] = token\n","\n","\n","# === –ü—Ä–æ–≤–µ—Ä—è–µ–º: –µ—Å–ª–∏ —Ä–µ–ø–æ–∑–∏—Ç–æ—Ä–∏–π –µ—â—ë –Ω–µ —Å–∫–∞—á–∞–Ω, –∫–ª–æ–Ω–∏—Ä—É–µ–º ===\n","if not os.path.exists(f\"{PROJECTS_DIR}/{REPO}/ML PART\"):\n","    print('–ó–∞–Ω–æ–≤–æ —Å–∫–ª–æ–Ω–∏—Ä–æ–≤–∞–ª–∏ —Ä–µ–ø—É')\n","    !git clone https://{USER}:{os.environ[\"GITHUB_TOKEN\"]}@github.com/{USER}/{REPO}.git\n","# === –ü–µ—Ä–µ—Ö–æ–¥–∏–º –≤ –ø–∞–ø–∫—É –ø—Ä–æ–µ–∫—Ç–∞ ===\n","%cd {REPO}/{'ML_PART'}\n","\n","# === –ù–∞—Å—Ç—Ä–æ–π–∫–∞ Git ===\n","!git config --global user.email \"{EMAIL}\"\n","!git config --global user.name \"{NAME}\"\n","!git remote set-url origin https://{USER}:{os.environ[\"GITHUB_TOKEN\"]}@github.com/{USER}/{REPO}.git\n","\n","print(\"‚úÖ –í—Å—ë –≥–æ—Ç–æ–≤–æ! –†–∞–±–æ—á–∞—è –ø–∞–ø–∫–∞:\", os.getcwd())\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_69974-nHwLK"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":13038,"status":"ok","timestamp":1759343752653,"user":{"displayName":"TVLG LX6500","userId":"15785277625807876484"},"user_tz":-180},"id":"43TOp5KPoqEB","outputId":"a3df1206-ede9-41af-e8e4-5175cf98cf4e"},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: torch>=2.8.0 in /usr/local/lib/python3.12/dist-packages (from -r requirements_bert.txt (line 2)) (2.8.0+cu126)\n","Requirement already satisfied: transformers>=4.35.2 in /usr/local/lib/python3.12/dist-packages (from -r requirements_bert.txt (line 3)) (4.56.1)\n","Requirement already satisfied: huggingface_hub>=0.20.3 in /usr/local/lib/python3.12/dist-packages (from -r requirements_bert.txt (line 6)) (0.35.0)\n","Requirement already satisfied: accelerate>=0.24.1 in /usr/local/lib/python3.12/dist-packages (from -r requirements_bert.txt (line 7)) (1.10.1)\n","Requirement already satisfied: tokenizers>=0.15.0 in /usr/local/lib/python3.12/dist-packages (from -r requirements_bert.txt (line 8)) (0.22.0)\n","Requirement already satisfied: datasets>=2.14.7 in /usr/local/lib/python3.12/dist-packages (from -r requirements_bert.txt (line 9)) (4.0.0)\n","Requirement already satisfied: pandas>=2.1.4 in /usr/local/lib/python3.12/dist-packages (from -r requirements_bert.txt (line 12)) (2.2.2)\n","Requirement already satisfied: numpy>=1.26.0 in /usr/local/lib/python3.12/dist-packages (from -r requirements_bert.txt (line 13)) (2.0.2)\n","Requirement already satisfied: matplotlib>=3.7.2 in /usr/local/lib/python3.12/dist-packages (from -r requirements_bert.txt (line 14)) (3.10.0)\n","Requirement already satisfied: scikit-learn>=1.3.2 in /usr/local/lib/python3.12/dist-packages (from -r requirements_bert.txt (line 15)) (1.6.1)\n","Collecting TorchCRF>=1.1.0 (from -r requirements_bert.txt (line 18))\n","  Downloading TorchCRF-1.1.0-py3-none-any.whl.metadata (2.3 kB)\n","Collecting seqeval>=1.2.2 (from -r requirements_bert.txt (line 21))\n","  Downloading seqeval-1.2.2.tar.gz (43 kB)\n","\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m43.6/43.6 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch>=2.8.0->-r requirements_bert.txt (line 2)) (3.19.1)\n","Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch>=2.8.0->-r requirements_bert.txt (line 2)) (4.15.0)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=2.8.0->-r requirements_bert.txt (line 2)) (75.2.0)\n","Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=2.8.0->-r requirements_bert.txt (line 2)) (1.13.3)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch>=2.8.0->-r requirements_bert.txt (line 2)) (3.5)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.8.0->-r requirements_bert.txt (line 2)) (3.1.6)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch>=2.8.0->-r requirements_bert.txt (line 2)) (2025.3.0)\n","Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.8.0->-r requirements_bert.txt (line 2)) (12.6.77)\n","Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.8.0->-r requirements_bert.txt (line 2)) (12.6.77)\n","Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=2.8.0->-r requirements_bert.txt (line 2)) (12.6.80)\n","Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=2.8.0->-r requirements_bert.txt (line 2)) (9.10.2.21)\n","Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.8.0->-r requirements_bert.txt (line 2)) (12.6.4.1)\n","Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=2.8.0->-r requirements_bert.txt (line 2)) (11.3.0.4)\n","Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.8.0->-r requirements_bert.txt (line 2)) (10.3.7.77)\n","Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.8.0->-r requirements_bert.txt (line 2)) (11.7.1.2)\n","Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.8.0->-r requirements_bert.txt (line 2)) (12.5.4.2)\n","Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.8.0->-r requirements_bert.txt (line 2)) (0.7.1)\n","Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch>=2.8.0->-r requirements_bert.txt (line 2)) (2.27.3)\n","Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.8.0->-r requirements_bert.txt (line 2)) (12.6.77)\n","Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=2.8.0->-r requirements_bert.txt (line 2)) (12.6.85)\n","Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=2.8.0->-r requirements_bert.txt (line 2)) (1.11.1.6)\n","Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch>=2.8.0->-r requirements_bert.txt (line 2)) (3.4.0)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from transformers>=4.35.2->-r requirements_bert.txt (line 3)) (25.0)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers>=4.35.2->-r requirements_bert.txt (line 3)) (6.0.2)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers>=4.35.2->-r requirements_bert.txt (line 3)) (2024.11.6)\n","Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from transformers>=4.35.2->-r requirements_bert.txt (line 3)) (2.32.4)\n","Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers>=4.35.2->-r requirements_bert.txt (line 3)) (0.6.2)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.12/dist-packages (from transformers>=4.35.2->-r requirements_bert.txt (line 3)) (4.67.1)\n","Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub>=0.20.3->-r requirements_bert.txt (line 6)) (1.1.10)\n","Requirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (from accelerate>=0.24.1->-r requirements_bert.txt (line 7)) (5.9.5)\n","Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets>=2.14.7->-r requirements_bert.txt (line 9)) (18.1.0)\n","Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from datasets>=2.14.7->-r requirements_bert.txt (line 9)) (0.3.8)\n","Requirement already satisfied: xxhash in /usr/local/lib/python3.12/dist-packages (from datasets>=2.14.7->-r requirements_bert.txt (line 9)) (3.5.0)\n","Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.12/dist-packages (from datasets>=2.14.7->-r requirements_bert.txt (line 9)) (0.70.16)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas>=2.1.4->-r requirements_bert.txt (line 12)) (2.9.0.post0)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas>=2.1.4->-r requirements_bert.txt (line 12)) (2025.2)\n","Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas>=2.1.4->-r requirements_bert.txt (line 12)) (2025.2)\n","Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.7.2->-r requirements_bert.txt (line 14)) (1.3.3)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.7.2->-r requirements_bert.txt (line 14)) (0.12.1)\n","Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.7.2->-r requirements_bert.txt (line 14)) (4.60.0)\n","Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.7.2->-r requirements_bert.txt (line 14)) (1.4.9)\n","Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.7.2->-r requirements_bert.txt (line 14)) (11.3.0)\n","Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.7.2->-r requirements_bert.txt (line 14)) (3.2.4)\n","Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn>=1.3.2->-r requirements_bert.txt (line 15)) (1.16.2)\n","Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn>=1.3.2->-r requirements_bert.txt (line 15)) (1.5.2)\n","Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn>=1.3.2->-r requirements_bert.txt (line 15)) (3.6.0)\n","Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.14.7->-r requirements_bert.txt (line 9)) (3.12.15)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas>=2.1.4->-r requirements_bert.txt (line 12)) (1.17.0)\n","Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->transformers>=4.35.2->-r requirements_bert.txt (line 3)) (3.4.3)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->transformers>=4.35.2->-r requirements_bert.txt (line 3)) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->transformers>=4.35.2->-r requirements_bert.txt (line 3)) (2.5.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->transformers>=4.35.2->-r requirements_bert.txt (line 3)) (2025.8.3)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=2.8.0->-r requirements_bert.txt (line 2)) (1.3.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=2.8.0->-r requirements_bert.txt (line 2)) (3.0.2)\n","Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.14.7->-r requirements_bert.txt (line 9)) (2.6.1)\n","Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.14.7->-r requirements_bert.txt (line 9)) (1.4.0)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.14.7->-r requirements_bert.txt (line 9)) (25.3.0)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.14.7->-r requirements_bert.txt (line 9)) (1.7.0)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.14.7->-r requirements_bert.txt (line 9)) (6.6.4)\n","Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.14.7->-r requirements_bert.txt (line 9)) (0.3.2)\n","Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.14.7->-r requirements_bert.txt (line 9)) (1.20.1)\n","Downloading TorchCRF-1.1.0-py3-none-any.whl (5.2 kB)\n","Building wheels for collected packages: seqeval\n","  Building wheel for seqeval (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for seqeval: filename=seqeval-1.2.2-py3-none-any.whl size=16162 sha256=ea1669fa1edaa8a26beeaff577463c17a29182ea96a6f4bb1f53df04ee984119\n","  Stored in directory: /root/.cache/pip/wheels/5f/b8/73/0b2c1a76b701a677653dd79ece07cfabd7457989dbfbdcd8d7\n","Successfully built seqeval\n","Installing collected packages: seqeval, TorchCRF\n","Successfully installed TorchCRF-1.1.0 seqeval-1.2.2\n"]}],"source":["# –£—Å—Ç–∞–Ω–æ–≤–∫–∞ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–µ–π\n","!pip install -r requirements_bert.txt\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":14158,"status":"ok","timestamp":1759343766813,"user":{"displayName":"TVLG LX6500","userId":"15785277625807876484"},"user_tz":-180},"id":"WKM-_g4rH0SC","outputId":"7d64de6e-523b-4f58-f38d-640348c99fe6"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting onnxruntime\n","  Downloading onnxruntime-1.23.0-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (4.9 kB)\n","Collecting coloredlogs (from onnxruntime)\n","  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl.metadata (12 kB)\n","Requirement already satisfied: flatbuffers in /usr/local/lib/python3.12/dist-packages (from onnxruntime) (25.2.10)\n","Requirement already satisfied: numpy>=1.21.6 in /usr/local/lib/python3.12/dist-packages (from onnxruntime) (2.0.2)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from onnxruntime) (25.0)\n","Requirement already satisfied: protobuf in /usr/local/lib/python3.12/dist-packages (from onnxruntime) (5.29.5)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.12/dist-packages (from onnxruntime) (1.13.3)\n","Collecting humanfriendly>=9.1 (from coloredlogs->onnxruntime)\n","  Downloading humanfriendly-10.0-py2.py3-none-any.whl.metadata (9.2 kB)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy->onnxruntime) (1.3.0)\n","Downloading onnxruntime-1.23.0-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (17.3 MB)\n","\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m17.3/17.3 MB\u001b[0m \u001b[31m64.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n","\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n","\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: humanfriendly, coloredlogs, onnxruntime\n","Successfully installed coloredlogs-15.0.1 humanfriendly-10.0 onnxruntime-1.23.0\n"]}],"source":["! pip install --upgrade onnxruntime"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":27604,"status":"ok","timestamp":1759343794435,"user":{"displayName":"TVLG LX6500","userId":"15785277625807876484"},"user_tz":-180},"id":"cU-8Ca9forrD","outputId":"43950f1d-5c34-449a-a6b9-79d8ff770508"},"outputs":[{"output_type":"stream","name":"stdout","text":["‚úÖ Transformers —É—Å–ø–µ—à–Ω–æ –∏–º–ø–æ—Ä—Ç–∏—Ä–æ–≤–∞–Ω—ã\n","‚úÖ TorchCRF —É—Å–ø–µ—à–Ω–æ –∏–º–ø–æ—Ä—Ç–∏—Ä–æ–≤–∞–Ω\n"]}],"source":["from google.colab import drive\n","import getpass, os, json, random, time\n","import numpy as np\n","import pandas as pd\n","import torch\n","import matplotlib.pyplot as plt\n","from sklearn.model_selection import train_test_split, KFold\n","from torch.utils.data import Dataset, DataLoader\n","from torch.optim import AdamW\n","\n","# –ò–º–ø–æ—Ä—Ç—ã transformers —Å –æ–±—Ä–∞–±–æ—Ç–∫–æ–π –æ—à–∏–±–æ–∫\n","try:\n","    from transformers import AutoTokenizer, AutoModelForTokenClassification, get_scheduler\n","    print(\"‚úÖ Transformers —É—Å–ø–µ—à–Ω–æ –∏–º–ø–æ—Ä—Ç–∏—Ä–æ–≤–∞–Ω—ã\")\n","except ImportError as e:\n","    print(f\"‚ùå –û—à–∏–±–∫–∞ –∏–º–ø–æ—Ä—Ç–∞ transformers: {e}\")\n","    !pip install transformers==4.35.2\n","    from transformers import AutoTokenizer, AutoModelForTokenClassification, get_scheduler\n","\n","try:\n","    from TorchCRF import CRF\n","    print(\"‚úÖ TorchCRF —É—Å–ø–µ—à–Ω–æ –∏–º–ø–æ—Ä—Ç–∏—Ä–æ–≤–∞–Ω\")\n","except ImportError as e:\n","    print(f\"‚ùå –û—à–∏–±–∫–∞ –∏–º–ø–æ—Ä—Ç–∞ TorchCRF: {e}\")\n","    !pip install TorchCRF==1.1.0\n","    from TorchCRF import CRF\n","\n","import ast\n","import traceback\n","from module import calculate_ner_metrics, calculate_macro_f1, evaluate_model, check_repo_exists,\\\n"," process_submission, parse_span_str, merge_prefixed_char_spans, tokenize_and_align_labels, \\\n"," token_labels_to_char_spans, build_label_maps_from_examples, HFWrapper, setup_hf_login, \\\n"," save_spacy_to_hf, list_my_repos, load_spacy_from_hf, NERModelWithCRF, save_bert_to_hf, load_bert_from_hf, process_submission_bert, ensemble_predict\n","from torch.nn.utils.rnn import pad_sequence"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bO4KcUNr0ASv"},"outputs":[],"source":["# --- –û—Å–Ω–æ–≤–Ω—ã–µ –ø—É—Ç–∏ –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ ---\n","WHERE_DATA = 'cleared_data'\n","BASE_MODEL_NAME = \"DeepPavlov/rubert-base-cased\"\n","OUT_DIR = f\"OUTPUT/{WHERE_DATA}/{BASE_MODEL_NAME}\"\n","os.makedirs(OUT_DIR, exist_ok=True)       # –ø–∞–ø–∫–∞ –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –≤—Å–µ—Ö —Ñ–∞–π–ª–æ–≤\n","FINAL_METRICS_PATH = f\"{OUT_DIR}/final_training_metrics_per_epoch.csv\"\n","MODEL_PATH = f'MODELS/{WHERE_DATA}/{BASE_MODEL_NAME}'\n","os.makedirs(MODEL_PATH, exist_ok=True)\n","DATA_DIR = f'data/{WHERE_DATA}/'\n","PATIENCE = 3      # –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —ç–ø–æ—Ö –±–µ–∑ —É–ª—É—á—à–µ–Ω–∏—è F1 –¥–æ –æ—Å—Ç–∞–Ω–æ–≤–∫–∏\n","SEED = 42\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":18280,"status":"ok","timestamp":1759343813513,"user":{"displayName":"TVLG LX6500","userId":"15785277625807876484"},"user_tz":-180},"id":"0KGUxpTbJQSH","outputId":"1ddc3861-2000-40ae-8896-8fcec7c38e34"},"outputs":[{"output_type":"stream","name":"stdout","text":["–í–≤–µ–¥–∏ HFT —Ç–æ–∫–µ–Ω: ¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑\n","‚úÖ –ê–≤—Ç–æ—Ä–∏–∑–∞—Ü–∏—è HF –Ω–∞—Å—Ç—Ä–æ–µ–Ω–∞\n"]},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":7}],"source":["\n","# Hugging Face –Ω–∞—Å—Ç—Ä–æ–π–∫–∏\n","HF_TOKEN= getpass.getpass('–í–≤–µ–¥–∏ HFT —Ç–æ–∫–µ–Ω: ')\n","HF_USERNAME = \"alexflex04\"\n","BERT_REPO_NAME = f\"{HF_USERNAME}/NER_{WHERE_DATA}_bert\"\n","\n","setup_hf_login(HF_TOKEN)"]},{"cell_type":"code","source":["CONFIG = {\n","    \"model_checkpoint\": BASE_MODEL_NAME,\n","    \"num_epochs\": 10,\n","    \"batch_size\": 64,\n","    \"learning_rate\": 2e-5,\n","    \"weight_decay\": 0.01,\n","    \"patience\": PATIENCE,\n","    \"max_length\": 128,\n","    \"label_list\": [\"O\", \"B-TYPE\", \"I-TYPE\", \"B-BRAND\", \"I-BRAND\", \"B-VOLUME\", \"I-VOLUME\", \"B-PERCENT\", \"I-PERCENT\"],\n","    \"id2label\": {i: label for i, label in enumerate([\"O\", \"B-TYPE\", \"I-TYPE\", \"B-BRAND\", \"I-BRAND\", \"B-VOLUME\", \"I-VOLUME\", \"B-PERCENT\", \"I-PERCENT\"])},\n","    \"label2id\": {label: i for i, label in enumerate([\"O\", \"B-TYPE\", \"I-TYPE\", \"B-BRAND\", \"I-BRAND\", \"B-VOLUME\", \"I-VOLUME\", \"B-PERCENT\", \"I-PERCENT\"])},\n","\n","}"],"metadata":{"id":"IHotQkii7_BW"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"t_VWEzvBF9nH"},"outputs":[],"source":["os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mFL3k8dl0FOh"},"outputs":[],"source":["random.seed(SEED)\n","np.random.seed(SEED)\n","torch.manual_seed(SEED)\n","torch.cuda.manual_seed_all(SEED)\n","torch.backends.cudnn.deterministic = True\n","torch.backends.cudnn.benchmark = False"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":189,"status":"ok","timestamp":1759343818139,"user":{"displayName":"TVLG LX6500","userId":"15785277625807876484"},"user_tz":-180},"id":"jgkUd-P9ov1X","outputId":"55d4315b-4cd6-4067-c7ea-a9f30ef903a7"},"outputs":[{"output_type":"stream","name":"stdout","text":["–ò—Å–ø–æ–ª—å–∑—É–µ–º–æ–µ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ: cuda\n"]}],"source":["device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","print(f\"–ò—Å–ø–æ–ª—å–∑—É–µ–º–æ–µ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ: {device}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4j7jeRhb0Zfz"},"outputs":[],"source":["# –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö\n","train_split = pd.read_csv(f\"{DATA_DIR}train.csv\")\n","valid_data = pd.read_csv(f\"{DATA_DIR}val.csv\")\n","\n","def parse_row_to_example(row):\n","    try:\n","        ann = ast.literal_eval(row['annotation'])\n","    except Exception:\n","        ann = []\n","    return (row['sample'],  ann)\n","\n","train_data = [parse_row_to_example(row) for _, row in train_split.iterrows()]\n","valid_data = [parse_row_to_example(row) for _, row in valid_data.iterrows()]\n","\n","from transformers import AutoTokenizer\n","\n","tokenizer = AutoTokenizer.from_pretrained(\"DeepPavlov/rubert-base-cased\", use_fast=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KaFuwbUZzP9n"},"outputs":[],"source":["# examples = [\n","#     (\"—è–π—Ü–æ –∫—É—Ä–∏–Ω–æ–µ\", [(0, 4, 'B-TYPE'), (5, 12, 'I-TYPE')]),\n","#     (\"—è–π—Ü–æ –∫—É—Ä–∏–Ω–æ–µ 30—à—Ç\", [(0,4,'B-TYPE'), (5,12,'I-TYPE'), (13,17,'B-VOLUME')]),\n","#     (\"—Å–æ–∫ 0.2 –ª 10%\", [(0,3,'B-TYPE'), (4,7,'B-VOLUME'), (8,9,'I-VOLUME'), (10,13,'B-PERCENT')]),\n","#     (\"–º–∞—Å–ª–æ —Å–ª–∏–≤–æ—á–Ω–æ–µ 72% 250 –≥ President\", [(0,5,'B-TYPE'), (6,15,'I-TYPE'), (16,19,'B-PERCENT'), (20,23,'B-VOLUME'), (24,25,'I-VOLUME'), (26,35,'B-BRAND')]),\n","#     (\"–ø–∏–≤–æ Baltika 4.8% 0.5 –ª\", [(0,4,'B-TYPE'), (5,12,'B-BRAND'), (13,17,'B-PERCENT'), (18,21,'B-VOLUME'), (22,23,'I-VOLUME')]),\n","#     (\"global village –ª–µ—Ç–Ω—è—è —è–≥–æ–¥–∞\", [(0,6,'B-BRAND'), (7,14,'I-BRAND'), (15,21,'B-TYPE'), (22,27,'I-TYPE')]),\n","#     (\"arkhangel'skkhleb –±–∞–≥–µ—Ç\", [(0,17,'B-BRAND'), (18,23,'B-TYPE')]),\n","#     (\"aunfed\", [(0,6,'O')]),\n","#     (\"bunk club\", [(0,4,'O'), (5,9,'O')]),\n","# ]\n","\n","# # Run tests\n","# for text, entities in examples:\n","#     print(f\"TEXT:{text} Entities:{entities}\")\n","#     parsed = entities  # if you had strings: parse_span_str(...)\n","#     enc = tokenize_and_align_labels(text, parsed, tokenizer, add_special_tokens=True)\n","#     tokens = enc['tokens']; offsets = enc['offsets']; tlabels = enc['token_labels']\n","#     print(\"–¢–æ–∫–µ–Ω—ã:\", tokens)\n","#     print(\"OFFSETS:\", offsets)\n","#     print(\"TOKEN LABELS:\", tlabels)\n","#     recovered = token_labels_to_char_spans(offsets, tlabels)\n","#     print(\"RECOVERED CHAR-SPANS:\", recovered)\n","#     merged = merge_prefixed_char_spans(parsed)\n","#     # prepare expected in recovered-format: B-<BASE> for entities, 'O' for O\n","#     expected = []\n","#     for s,e,lab in merged:\n","#         if lab == 'O':\n","#             expected.append((s,e,'O'))\n","#         else:\n","#             expected.append((s,e,'B-' + lab))\n","#     print(\"EXPECTED (merged):\", expected)\n","#     ok = (recovered == expected)\n","#     print(\"ROUND-TRIP OK:\", ok)\n","#     if not ok:\n","#         print(\"NOTE: mismatches can happen if tokenizer splits differently; inspect offsets and labels.\")\n","#     print(\"-\" * 60)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ViwlaKNrFIkC"},"outputs":[],"source":["class NERDataset(Dataset):\n","    def __init__(self, data, tokenizer, label2id):\n","        self.data = data\n","        self.tokenizer = tokenizer\n","        self.label2id = label2id\n","\n","    def __len__(self):\n","        return len(self.data)\n","\n","    def __getitem__(self, idx):\n","        text, annotations = self.data[idx]\n","        entities = annotations\n","\n","        # –î–æ–±–∞–≤–ª—è–µ–º –ø–∞—Ä–∞–º–µ—Ç—Ä—ã truncation –∏ max_length –≤ –≤—ã–∑–æ–≤ tokenize_and_align_labels\n","        # (–ø—Ä–µ–¥–ø–æ–ª–∞–≥–∞—è, —á—Ç–æ —Ñ—É–Ω–∫—Ü–∏—è tokenize_and_align_labels –ø—Ä–∏–Ω–∏–º–∞–µ—Ç –∏—Ö; –µ—Å–ª–∏ –Ω–µ—Ç, –¥–æ–±–∞–≤—å—Ç–µ –≤ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Ñ—É–Ω–∫—Ü–∏–∏)\n","        tokenized = tokenize_and_align_labels(\n","            text,\n","            entities,\n","            self.tokenizer,\n","            add_special_tokens=True,\n","            truncation=True,\n","            max_length=CONFIG[\"max_length\"]  # –ò—Å–ø–æ–ª—å–∑—É–µ–º CONFIG –∏–∑ –≥–ª–æ–±–∞–ª—å–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞\n","        )\n","\n","        # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º —Ç–µ–∫—Å—Ç–æ–≤—ã–µ –º–µ—Ç–∫–∏ –≤ —á–∏—Å–ª–æ–≤—ã–µ ID\n","        labels = [self.label2id[label] for label in tokenized['token_labels']]\n","\n","        # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –≤ —Ç–µ–Ω–∑–æ—Ä—ã\n","        input_ids = torch.tensor(tokenized['input_ids'])\n","        attention_mask = torch.tensor([1] * len(tokenized['input_ids']))\n","        labels = torch.tensor(labels)\n","\n","        # –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º -100 –¥–ª—è [CLS] (index 0) –∏ [SEP] (last index), —á—Ç–æ–±—ã –∏–≥–Ω–æ—Ä–∏—Ä–æ–≤–∞—Ç—å –∏—Ö –≤ –ª–æ—Å—Å–µ\n","        labels[0] = -100\n","        labels[-1] = -100\n","\n","        return {\n","            'input_ids': input_ids,\n","            'attention_mask': attention_mask,\n","            'labels': labels\n","        }\n","\n","def collate_fn(batch):\n","    input_ids = [item[\"input_ids\"] for item in batch]\n","    attention_mask = [item[\"attention_mask\"] for item in batch]\n","    labels = [item[\"labels\"] for item in batch]\n","\n","    input_ids = pad_sequence(input_ids, batch_first=True, padding_value=0)\n","    attention_mask = pad_sequence(attention_mask, batch_first=True, padding_value=0)\n","    labels = pad_sequence(labels, batch_first=True, padding_value=-100)  # -100 –¥–ª—è –∏–≥–Ω–æ—Ä–∏—Ä–æ–≤–∞–Ω–∏—è –≤ loss\n","\n","    return {\n","        \"input_ids\": input_ids,\n","        \"attention_mask\": attention_mask,\n","        \"labels\": labels\n","    }\n","\n","\n","\n","def evaluate_model(model, eval_data, tokenizer, id2label):\n","    entity_pairs = []\n","    model.eval()\n","    # print(eval_data)\n","\n","    for text, entities in eval_data:\n","\n","        # print(text, entities)\n","\n","        tokenized = tokenizer(\n","            [text],\n","            padding=True,\n","            truncation=True,\n","            max_length=512,\n","            return_tensors=\"pt\",\n","            return_offsets_mapping=True\n","        )\n","\n","        device = next(model.parameters()).device\n","        input_ids = tokenized[\"input_ids\"].to(device)\n","        attention_mask = tokenized[\"attention_mask\"].to(device)\n","\n","        with torch.no_grad():\n","            # –ü–æ–ª—É—á–∞–µ–º emissions\n","            emissions = model.get_emissions(input_ids, attention_mask)\n","            # –ò—Å–ø–æ–ª—å–∑—É–µ–º argmax –¥–ª—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è —Ç–µ–≥–æ–≤\n","            pred = torch.argmax(emissions, dim=-1)[0]\n","\n","        bio_labels = [id2label[p.item()] for p in pred]\n","        offsets = tokenized[\"offset_mapping\"][0].tolist()\n","        pred_entities = token_labels_to_char_spans(offsets, bio_labels)\n","        true_entities = entities\n","        entity_pairs.append((true_entities, pred_entities))\n","\n","    macro_f1, f1_type, f1_brand, f1_volume, f1_percent = calculate_macro_f1(entity_pairs)\n","    return {\n","        'f1_macro': macro_f1,\n","        'f1_TYPE': f1_type,\n","        'f1_BRAND': f1_brand,\n","        'f1_VOLUME': f1_volume,\n","        'f1_PERCENT': f1_percent\n","    }"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1786,"status":"ok","timestamp":1759347007573,"user":{"displayName":"TVLG LX6500","userId":"15785277625807876484"},"user_tz":-180},"id":"92aearUZ0hEQ","outputId":"9d346d45-51c4-4645-ad9c-ca8c66c57dab"},"outputs":[{"output_type":"stream","name":"stdout","text":["=== –ò–ù–ò–¶–ò–ê–õ–ò–ó–ê–¶–ò–Ø –ú–û–î–ï–õ–ò ===\n"]},{"output_type":"stream","name":"stderr","text":["Some weights of BertForTokenClassification were not initialized from the model checkpoint at DeepPavlov/rubert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"output_type":"stream","name":"stdout","text":["‚úÖ –ú–æ–¥–µ–ª—å –∏ –¥–∞–Ω–Ω—ã–µ —É—Å–ø–µ—à–Ω–æ –ø–æ–¥–≥–æ—Ç–æ–≤–ª–µ–Ω—ã!\n","–†–∞–∑–º–µ—Ä –æ–±—É—á–∞—é—â–µ–π –≤—ã–±–æ—Ä–∫–∏: 21826\n"]}],"source":["print(\"=== –ò–ù–ò–¶–ò–ê–õ–ò–ó–ê–¶–ò–Ø –ú–û–î–ï–õ–ò ===\")\n","model = NERModelWithCRF(len(CONFIG[\"label_list\"])).to(device)\n","optimizer = AdamW(model.parameters(), lr=CONFIG[\"learning_rate\"], weight_decay=CONFIG[\"weight_decay\"])\n","num_training_steps = CONFIG[\"num_epochs\"] * len(train_data) // CONFIG[\"batch_size\"]\n","scheduler = get_scheduler(\"linear\", optimizer=optimizer, num_warmup_steps=0, num_training_steps=num_training_steps)\n","\n","train_dataset = NERDataset(train_data, tokenizer, label2id=CONFIG['label2id'])\n","train_loader = DataLoader(train_dataset, batch_size=CONFIG[\"batch_size\"], shuffle=True, collate_fn=collate_fn)\n","\n","\n","metrics_df = pd.DataFrame(columns=['epoch', 'loss', 'f1_macro', 'f1_TYPE', 'f1_BRAND', 'f1_VOLUME', 'f1_PERCENT'])\n","best_f1 = 0\n","patience_counter = 0\n","best_epoch = 0\n","\n","print(\"‚úÖ –ú–æ–¥–µ–ª—å –∏ –¥–∞–Ω–Ω—ã–µ —É—Å–ø–µ—à–Ω–æ –ø–æ–¥–≥–æ—Ç–æ–≤–ª–µ–Ω—ã!\")\n","print(f\"–†–∞–∑–º–µ—Ä –æ–±—É—á–∞—é—â–µ–π –≤—ã–±–æ—Ä–∫–∏: {len(train_dataset)}\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4,"status":"ok","timestamp":1759345723922,"user":{"displayName":"TVLG LX6500","userId":"15785277625807876484"},"user_tz":-180},"id":"-DUbDrZMTLlP","outputId":"8f36f82c-b48f-4ca0-8417-3562b2fe12cf"},"outputs":[{"output_type":"stream","name":"stdout","text":["NERModelWithCRF(\n","  (bert): BertForTokenClassification(\n","    (bert): BertModel(\n","      (embeddings): BertEmbeddings(\n","        (word_embeddings): Embedding(119547, 768, padding_idx=0)\n","        (position_embeddings): Embedding(512, 768)\n","        (token_type_embeddings): Embedding(2, 768)\n","        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","        (dropout): Dropout(p=0.1, inplace=False)\n","      )\n","      (encoder): BertEncoder(\n","        (layer): ModuleList(\n","          (0-11): 12 x BertLayer(\n","            (attention): BertAttention(\n","              (self): BertSdpaSelfAttention(\n","                (query): Linear(in_features=768, out_features=768, bias=True)\n","                (key): Linear(in_features=768, out_features=768, bias=True)\n","                (value): Linear(in_features=768, out_features=768, bias=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (output): BertSelfOutput(\n","                (dense): Linear(in_features=768, out_features=768, bias=True)\n","                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","            )\n","            (intermediate): BertIntermediate(\n","              (dense): Linear(in_features=768, out_features=3072, bias=True)\n","              (intermediate_act_fn): GELUActivation()\n","            )\n","            (output): BertOutput(\n","              (dense): Linear(in_features=3072, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","        )\n","      )\n","    )\n","    (dropout): Dropout(p=0.1, inplace=False)\n","    (classifier): Linear(in_features=768, out_features=9, bias=True)\n","  )\n","  (crf): CRF()\n",")\n","True\n"]}],"source":["print(model)  # –ø–æ—Å–º–æ—Ç—Ä–∏—Ç–µ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—É\n","print(hasattr(model, 'crf'))  # –ø—Ä–æ–≤–µ—Ä—å—Ç–µ –Ω–∞–ª–∏—á–∏–µ CRF —Å–ª–æ—è\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8IyWSwnO0jQ4","outputId":"54fc405f-8258-4a0e-d894-9f8345148339"},"outputs":[{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["\n","=== –ù–ê–ß–ê–õ–û SCREENING –û–ë–£–ß–ï–ù–ò–Ø ===\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["/tmp/ipython-input-1198690905.py:30: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n","  metrics_df = pd.concat([metrics_df, pd.DataFrame([metrics_row])], ignore_index=True)\n"]},{"output_type":"stream","name":"stdout","text":["–≠–ø–æ—Ö–∞ 1   | Loss: 1.4249 | F1-macro: 0.5000 | F1-TYPE: 0.7242 | F1-BRAND: 0.7959 | F1-VOLUME: 0.0800 | F1-PERCENT: 0.4000\n","–≠–ø–æ—Ö–∞ 2   | Loss: 0.6208 | F1-macro: 0.5676 | F1-TYPE: 0.7265 | F1-BRAND: 0.8254 | F1-VOLUME: 0.1852 | F1-PERCENT: 0.5333\n","–≠–ø–æ—Ö–∞ 3   | Loss: 0.4289 | F1-macro: 0.5870 | F1-TYPE: 0.7322 | F1-BRAND: 0.8235 | F1-VOLUME: 0.2041 | F1-PERCENT: 0.5882\n"]}],"source":["print(\"\\n=== –ù–ê–ß–ê–õ–û SCREENING –û–ë–£–ß–ï–ù–ò–Ø ===\")\n","try:\n","    for epoch in range(CONFIG[\"num_epochs\"]):\n","        model.train()\n","        total_loss = 0\n","        for batch in train_loader:\n","            input_ids = batch[\"input_ids\"].to(device)\n","            attention_mask = batch[\"attention_mask\"].to(device)\n","            labels = batch[\"labels\"].to(device)\n","            loss = model(input_ids, attention_mask, labels)\n","\n","            # –£—Å—Ä–µ–¥–Ω—è–µ–º –ª–æ—Å—Å –ø–æ –±–∞—Ç—á—É –∏ –∑–∞—Ç–µ–º –ø–æ–ª—É—á–∞–µ–º —Å–∫–∞–ª—è—Ä\n","            loss_mean = loss.mean()  # –î–æ–±–∞–≤–ª—è–µ–º —ç—Ç—É —Å—Ç—Ä–æ–∫—É\n","            total_loss += loss_mean.item()  # –ò—Å–ø–æ–ª—å–∑—É–µ–º —É—Å—Ä–µ–¥–Ω–µ–Ω–Ω–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ\n","\n","            optimizer.zero_grad()\n","            loss_mean.backward()  # –ò—Å–ø–æ–ª—å–∑—É–µ–º —É—Å—Ä–µ–¥–Ω–µ–Ω–Ω–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ –¥–ª—è –æ–±—Ä–∞—Ç–Ω–æ–≥–æ —Ä–∞—Å–ø—Ä–æ—Å—Ç—Ä–∞–Ω–µ–Ω–∏—è\n","            optimizer.step()\n","            scheduler.step()\n","\n","        avg_loss = total_loss / len(train_loader)\n","        eval_metrics = evaluate_model(model, valid_data, tokenizer, id2label=CONFIG['id2label'])\n","        current_f1 = eval_metrics[\"f1_macro\"]\n","\n","        metrics_row = {\n","            'epoch': epoch + 1,\n","            'loss': avg_loss,\n","            **eval_metrics\n","        }\n","        metrics_df = pd.concat([metrics_df, pd.DataFrame([metrics_row])], ignore_index=True)\n","\n","        print(f'–≠–ø–æ—Ö–∞ {epoch + 1:<3} | Loss: {avg_loss:.4f} | '\n","              f'F1-macro: {current_f1:.4f} | '\n","              f'F1-TYPE: {eval_metrics[\"f1_TYPE\"]:.4f} | '\n","              f'F1-BRAND: {eval_metrics[\"f1_BRAND\"]:.4f} | '\n","              f'F1-VOLUME: {eval_metrics[\"f1_VOLUME\"]:.4f} | '\n","              f'F1-PERCENT: {eval_metrics[\"f1_PERCENT\"]:.4f}')\n","\n","        if current_f1 > best_f1:\n","            best_f1 = current_f1\n","            best_epoch = epoch + 1\n","            patience_counter = 0\n","        else:\n","            patience_counter += 1\n","            print(f\"‚è≥ Patience: {patience_counter}/{PATIENCE}\")\n","            if patience_counter >= PATIENCE:\n","                print(f\"\\nüõë –†–∞–Ω–Ω—è—è –æ—Å—Ç–∞–Ω–æ–≤–∫–∞ –Ω–∞ —ç–ø–æ—Ö–µ {epoch + 1}\")\n","                print(f\"–õ—É—á—à–∏–π F1-macro: {best_f1:.4f} –¥–æ—Å—Ç–∏–≥–Ω—É—Ç –Ω–∞ —ç–ø–æ—Ö–µ {best_epoch}\")\n","                break\n","\n","except Exception as e:\n","    print(f'üí• –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞: {str(e)}')\n","    print(traceback.format_exc())\n","\n","finally:\n","    # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ screening –º–æ–¥–µ–ª–∏ –Ω–∞ HF\n","    print(f\"\\nüíæ –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ screening –º–æ–¥–µ–ª–∏ –Ω–∞ HF: {BERT_REPO_NAME+'_screening'}\")\n","    success = save_bert_to_hf(model, tokenizer, CONFIG, BERT_REPO_NAME+'_screening', HF_TOKEN)\n","\n","    if success:\n","        print(f\"üéâ BERT screening –º–æ–¥–µ–ª—å —É—Å–ø–µ—à–Ω–æ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –Ω–∞ HF: {BERT_REPO_NAME+'_screening'}\")\n","    else:\n","        print(\"‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å —Å–æ—Ö—Ä–∞–Ω–∏—Ç—å BERT screening –º–æ–¥–µ–ª—å –Ω–∞ HF\")\n","\n","    # –õ–æ–∫–∞–ª—å–Ω–æ–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ\n","    # torch.save(model.state_dict(), f\"{MODEL_PATH}/model_screening.pt\")\n","    metrics_df.to_csv(f'{OUT_DIR}/screening_metrics.csv', index=False)\n","    print(\"üíæ–ú–µ—Ç—Ä–∏–∫–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –ª–æ–∫–∞–ª—å–Ω–æ\")\n","\n","print(\"\\n\" + \"=\"*80)\n","print(\"–ò–¢–û–ì–û–í–´–ï –†–ï–ó–£–õ–¨–¢–ê–¢–´ SCREENING:\")\n","print(\"=\"*80)\n","print(f\"–õ—É—á—à–∏–π F1-macro: {best_f1:.4f} –Ω–∞ —ç–ø–æ—Ö–µ {best_epoch}\")\n","print(f\"–í—Å–µ–≥–æ —ç–ø–æ—Ö –≤—ã–ø–æ–ª–Ω–µ–Ω–æ: {len(metrics_df)}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"a8NbgDCk0mEW"},"outputs":[],"source":["# –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è\n","plt.figure(figsize=(15, 10))\n","plt.subplot(2, 1, 1)\n","plt.plot(metrics_df['epoch'], metrics_df['loss'], 'b-', linewidth=2, label='Loss')\n","plt.axvline(x=best_epoch, color='r', linestyle='--', alpha=0.7, label=f'–õ—É—á—à–∞—è —ç–ø–æ—Ö–∞ ({best_epoch})')\n","plt.xlabel('–≠–ø–æ—Ö–∞')\n","plt.ylabel('Loss')\n","plt.title('Loss –ø–æ —ç–ø–æ—Ö–∞–º')\n","plt.legend()\n","plt.grid(True, alpha=0.3)\n","\n","plt.subplot(2, 1, 2)\n","plt.plot(metrics_df['epoch'], metrics_df['f1_macro'], 'r-', linewidth=3, label='F1-macro')\n","plt.plot(metrics_df['epoch'], metrics_df['f1_TYPE'], 'g--', label='F1-TYPE')\n","plt.plot(metrics_df['epoch'], metrics_df['f1_BRAND'], 'b--', label='F1-BRAND')\n","plt.plot(metrics_df['epoch'], metrics_df['f1_VOLUME'], 'y--', label='F1-VOLUME')\n","plt.plot(metrics_df['epoch'], metrics_df['f1_PERCENT'], 'c--', label='F1-PERCENT')\n","plt.axvline(x=best_epoch, color='r', linestyle='--', alpha=0.7, label=f'–õ—É—á—à–∞—è —ç–ø–æ—Ö–∞ ({best_epoch})')\n","plt.xlabel('–≠–ø–æ—Ö–∞')\n","plt.ylabel('F1 Score')\n","plt.title('F1 Scores –ø–æ —ç–ø–æ—Ö–∞–º')\n","plt.legend()\n","plt.grid(True, alpha=0.3)\n","\n","plt.tight_layout()\n","plt.savefig(f\"{OUT_DIR}/screening_metrics.png\", dpi=300, bbox_inches='tight')\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"oVXcgPQDKOoV"},"outputs":[],"source":["print(\"\\n=== –ü–†–û–í–ï–†–ö–ê –ó–ê–ì–†–£–ó–ö–ò –ú–û–î–ï–õ–ò ===\")\n","loaded_model, loaded_tokenizer, loaded_config = load_bert_from_hf(BERT_REPO_NAME+'_screening', HF_TOKEN, device)\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ivHG4pUMDQdc"},"outputs":[],"source":["if loaded_model:\n","    print(\"‚úÖ –ú–æ–¥–µ–ª—å —É—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω–∞ —Å HF!\")\n","    test_text = \"–º–æ–ª–æ–∫–æ –ü—Ä–æ—Å—Ç–æ–∫–≤–∞—à–∏–Ω–æ 2.5% 1–ª\"\n","    from module import HFWrapper\n","    wrapper = HFWrapper(loaded_model, loaded_tokenizer, id2label=CONFIG['id2label'])\n","    doc = wrapper(test_text)\n","    entities = [(ent.start_char, ent.end_char, ent.label_) for ent in doc.ents]\n","    print(f\"–¢–µ—Å—Ç–æ–≤—ã–π —Ç–µ–∫—Å—Ç: '{test_text}'\")\n","    print(f\"–ò–∑–≤–ª–µ—á–µ–Ω–Ω—ã–µ —Å—É—â–Ω–æ—Å—Ç–∏: {entities}\")\n","\n","    # –û–±—Ä–∞–±–æ—Ç–∫–∞ submission —Ñ–∞–π–ª–∞\n","    print(f\"\\n=== –û–ë–†–ê–ë–û–¢–ö–ê SUBMISSION –§–ê–ô–õ–ê ===\")\n","    process_submission_bert(\n","        model=loaded_model,\n","        tokenizer=loaded_tokenizer,\n","        input_file=os.getcwd()+'/data/submission.csv',\n","        output_file=f\"{OUT_DIR}/submission_screening.csv\",\n","        id2label=CONFIG['id2label']\n","    )\n","else:\n","    print(\"‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å –º–æ–¥–µ–ª—å –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è\")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":8,"status":"ok","timestamp":1759274683830,"user":{"displayName":"–ê–ª–µ–∫—Å–∞–Ω–¥—Ä –§–µ–¥–æ—Ä–æ–≤","userId":"11558309455709398811"},"user_tz":-180},"id":"cM7CtsGwKO5w","outputId":"de8afc09-dc7e-45da-dd04-5ef2b2124e4f"},"outputs":[{"name":"stdout","output_type":"stream","text":["[]\n"]}],"source":["print(doc.ents)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6vKo5oFn0s2V"},"outputs":[],"source":["# –Ø—á–µ–π–∫–∞ 2: –ü–æ–¥–±–æ—Ä –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ (Tuning) —Å grid search\n","# PARAM_GRID = {\n","#     \"learning_rate\": [1e-5, 2e-5, 3e-5],\n","#     \"batch_size\": [32, 64],\n","#     \"epochs\": [10, 20],\n","#     \"weight_decay\": [0.01, 0.1]\n","# }\n","\n","\n","PARAM_GRID = {\n","    \"learning_rate\": [ 3e-5],\n","    \"batch_size\": [64],\n","    \"epochs\": [3],\n","    \"weight_decay\": [ 0.1]\n","}\n","grid_results = []\n","\n","for lr in PARAM_GRID[\"learning_rate\"]:\n","    for bsz in PARAM_GRID[\"batch_size\"]:\n","        for max_ep in PARAM_GRID[\"epochs\"]:\n","            for wd in PARAM_GRID[\"weight_decay\"]:\n","                combo = {\"learning_rate\": lr, \"batch_size\": bsz, \"epochs\": max_ep, \"weight_decay\": wd}\n","                print(f\"\\n=== Tuning combo: learning_rate={lr}, batch_size={bsz}, epochs={max_ep}, weight_decay={wd} ===\")\n","\n","                model = NERModelWithCRF(len(CONFIG[\"label_list\"])).to(device)\n","                optimizer = AdamW(model.parameters(), lr=lr, weight_decay=wd)\n","                num_training_steps = max_ep * len(train_data) // bsz\n","                scheduler = get_scheduler(\"linear\", optimizer=optimizer, num_warmup_steps=0, num_training_steps=num_training_steps)\n","\n","                train_loader = DataLoader(train_dataset, batch_size=bsz, shuffle=True)\n","\n","                patience_counter, best_f1, best_metrics = 0, 0.0, None\n","                for epoch in range(1, max_ep + 1):\n","                    model.train()\n","                    total_loss = 0\n","                    for batch in train_loader:\n","                        input_ids = batch[\"input_ids\"].to(device)\n","                        attention_mask = batch[\"attention_mask\"].to(device)\n","                        labels = batch[\"labels\"].to(device)\n","                        loss = model(input_ids, attention_mask, labels)\n","                        loss_mean = loss.mean()\n","                        total_loss += loss_mean.item()\n","                        optimizer.zero_grad()\n","                        loss_mean.backward()\n","                        optimizer.step()\n","                        scheduler.step()\n","\n","                    avg_loss = total_loss / len(train_loader)\n","                    metrics = evaluate_model(model, valid_data, tokenizer)\n","                    metrics[\"epoch\"] = int(epoch)\n","                    metrics[\"loss\"] = float(avg_loss)\n","                    current_f1 = metrics[\"f1_macro\"]\n","\n","                    print(f\"Ep {epoch} | Loss: {metrics['loss']:.4f} | F1-macro: {current_f1:.4f}\")\n","\n","                    if current_f1 > best_f1:\n","                        best_f1 = current_f1\n","                        best_metrics = metrics\n","                        patience_counter = 0\n","                    else:\n","                        patience_counter += 1\n","                        if patience_counter >= PATIENCE:\n","                            break\n","\n","                combo[\"best_f1_macro\"] = best_f1\n","                combo[\"best_metrics\"] = best_metrics\n","                grid_results.append(combo)\n","\n","# –í—ã–±–æ—Ä –ª—É—á—à–∏—Ö –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤\n","best_combo = max(grid_results, key=lambda x: x[\"best_f1_macro\"])\n","print(\"\\nBest tuning params:\", best_combo)\n","\n","# –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤\n","pd.DataFrame(grid_results).to_csv(f\"{OUT_DIR}/tuning_summary.csv\", index=False)\n","with open(f\"{OUT_DIR}/tuning_detailed.json\", \"w\", encoding=\"utf-8\") as f:\n","    json.dump(grid_results, f, ensure_ascii=False, indent=2)\n","with open(f\"{OUT_DIR}/best_combo.json\", \"w\", encoding=\"utf-8\") as f:\n","    json.dump({k: v for k, v in best_combo.items() if k != \"best_metrics\"}, f, ensure_ascii=False, indent=2)\n","print(\"üíæ Tuning —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –∏ best_combo —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gU0AhvFT0tZq"},"outputs":[],"source":["# –Ø—á–µ–π–∫–∞ 3: –ö—Ä–æ—Å—Å-–≤–∞–ª–∏–¥–∞—Ü–∏—è (CV) —Å –ª—É—á—à–∏–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏\n","with open(f\"{OUT_DIR}/best_combo.json\", \"r\", encoding=\"utf-8\") as f:\n","    best_combo = json.load(f)\n","\n","best_lr = best_combo[\"learning_rate\"]\n","best_bsz = best_combo[\"batch_size\"]\n","best_max_ep = best_combo[\"epochs\"]\n","best_wd = best_combo[\"weight_decay\"]\n","\n","kf = KFold(n_splits=5, shuffle=True, random_state=SEED)\n","cv_results = []\n","fold_best_f1s = []\n","\n","for fold, (tr_idx, val_idx) in enumerate(kf.split(train_data), 1):\n","    print(f\"\\n=== CV Fold {fold} ===\")\n","    fold_train = [train_data[i] for i in tr_idx]\n","    fold_valid = [train_data[i] for i in val_idx]\n","\n","    fold_train_dataset = NERDataset(fold_train, tokenizer)\n","    fold_train_loader = DataLoader(fold_train_dataset, batch_size=best_bsz, shuffle=True)\n","\n","    model = NERModelWithCRF(len(CONFIG[\"label_list\"])).to(device)\n","    optimizer = AdamW(model.parameters(), lr=best_lr, weight_decay=best_wd)\n","    num_training_steps = best_max_ep * len(fold_train) // best_bsz\n","    scheduler = get_scheduler(\"linear\", optimizer=optimizer, num_warmup_steps=0, num_training_steps=num_training_steps)\n","\n","    patience_counter, best_f1, best_metrics = 0, 0.0, None\n","    for epoch in range(1, best_max_ep + 1):\n","        model.train()\n","        total_loss = 0\n","        for batch in fold_train_loader:\n","            input_ids = batch[\"input_ids\"].to(device)\n","            attention_mask = batch[\"attention_mask\"].to(device)\n","            labels = batch[\"labels\"].to(device)\n","            loss = model(input_ids, attention_mask, labels)\n","            loss_mean = loss.mean()\n","            total_loss += loss_mean.item()\n","            optimizer.zero_grad()\n","            loss_mean.backward()\n","            optimizer.step()\n","            scheduler.step()\n","\n","        avg_loss = total_loss / len(fold_train_loader)\n","        metrics = evaluate_model(model, fold_valid, tokenizer)\n","        metrics[\"epoch\"] = epoch\n","        metrics[\"loss\"] = avg_loss\n","        current_f1 = metrics[\"f1_macro\"]\n","\n","        print(f\"Fold {fold} Ep {epoch} | Loss: {metrics['loss']:.4f} | F1-macro: {current_f1:.4f}\")\n","\n","        if current_f1 > best_f1:\n","            best_f1 = current_f1\n","            best_metrics = metrics\n","            patience_counter = 0\n","        else:\n","            patience_counter += 1\n","            if patience_counter >= PATIENCE:\n","                break\n","\n","    cv_results.append({\"fold\": fold, \"best_f1_macro\": best_f1, \"best_metrics\": best_metrics})\n","    fold_best_f1s.append(best_f1)\n","\n","mean_f1 = np.mean(fold_best_f1s)\n","std_f1 = np.std(fold_best_f1s)\n","print(f\"\\nCV Results: Mean F1_macro = {mean_f1:.4f} ¬± {std_f1:.4f}\")\n","\n","# –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ CV\n","pd.DataFrame(cv_results).to_csv(f\"{OUT_DIR}/cv_summary.csv\", index=False)\n","with open(f\"{OUT_DIR}/cv_detailed.json\", \"w\", encoding=\"utf-8\") as f:\n","    json.dump(cv_results, f, ensure_ascii=False, indent=2)\n","print(\"üíæ CV —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Wz7v_fJW0ycg"},"outputs":[],"source":["# === –°—Ç—Ä–∞—Ç–µ–≥–∏—è 1: –£—Å—Ä–µ–¥–Ω—ë–Ω–Ω–∞—è –º–æ–¥–µ–ª—å ===\n","print(\"\\n=== –°—Ç—Ä–∞—Ç–µ–≥–∏—è 1: –§–∏–Ω–∞–ª—å–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ (—É—Å—Ä–µ–¥–Ω—ë–Ω–Ω–∞—è –º–æ–¥–µ–ª—å) ===\")\n","train_val = train_data + valid_data\n","train_val_dataset = NERDataset(train_val, tokenizer)\n","train_val_loader = DataLoader(train_val_dataset, batch_size=best_bsz, shuffle=True)\n","\n","model = NERModelWithCRF(len(CONFIG[\"label_list\"])).to(device)\n","optimizer = AdamW(model.parameters(), lr=best_lr, weight_decay=best_wd)\n","num_training_steps = best_max_ep * len(train_val) // best_bsz\n","scheduler = get_scheduler(\"linear\", optimizer=optimizer, num_warmup_steps=0, num_training_steps=num_training_steps)\n","\n","for epoch in range(1, best_max_ep + 1):\n","    model.train()\n","    total_loss = 0\n","    for batch in train_val_loader:\n","        input_ids = batch[\"input_ids\"].to(device)\n","        attention_mask = batch[\"attention_mask\"].to(device)\n","        labels = batch[\"labels\"].to(device)\n","        loss = model(input_ids, attention_mask, labels)\n","        loss_mean = loss.mean()\n","        total_loss += loss_mean.item()\n","        optimizer.zero_grad()\n","        loss_mean.backward()\n","        optimizer.step()\n","        scheduler.step()\n","\n","    avg_loss = total_loss / len(train_val_loader)\n","    print(f\"–≠–ø–æ—Ö–∞ {epoch} | Loss: {avg_loss:.4f}\")\n","\n","print(f\"\\nüíæ –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —É—Å—Ä–µ–¥–Ω—ë–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏ –Ω–∞ HF: {BERT_REPO_NAME}_mean\")\n","success = save_bert_to_hf(model, tokenizer, CONFIG, f\"{BERT_REPO_NAME}_mean\", HF_TOKEN)\n","if success:\n","    print(f\"üéâ BERT —É—Å—Ä–µ–¥–Ω—ë–Ω–Ω–∞—è –º–æ–¥–µ–ª—å —É—Å–ø–µ—à–Ω–æ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –Ω–∞ HF\")\n","else:\n","    print(\"‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å —Å–æ—Ö—Ä–∞–Ω–∏—Ç—å BERT —É—Å—Ä–µ–¥–Ω—ë–Ω–Ω—É—é –º–æ–¥–µ–ª—å\")\n"]},{"cell_type":"code","source":["# === –°—Ç—Ä–∞—Ç–µ–≥–∏—è 2: –ê–Ω—Å–∞–º–±–ª—å –º–æ–¥–µ–ª–µ–π ===\n","print(\"\\n=== –°—Ç—Ä–∞—Ç–µ–≥–∏—è 2: –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–µ–π —Ñ–æ–ª–¥–æ–≤ –¥–ª—è –∞–Ω—Å–∞–º–±–ª—è ===\")\n","for i in range(6):\n","  success = save_fold_models_to_hf(fold_models, tokenizer, f'{BERT_REPO_NAME}_{i+1}', HF_TOKEN, is_spacy=False)\n","  if success:\n","      print(f\"üéâ –ú–æ–¥–µ–ª–∏ —Ñ–æ–ª–¥–æ–≤ —É—Å–ø–µ—à–Ω–æ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –Ω–∞ HF\")\n","  else:\n","      print(\"‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å —Å–æ—Ö—Ä–∞–Ω–∏—Ç—å –º–æ–¥–µ–ª–∏ —Ñ–æ–ª–¥–æ–≤\")\n"],"metadata":{"id":"BpUWEO0A1H4X"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# === –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ ===\n","print(\"\\n=== –ü–†–û–í–ï–†–ö–ê –ó–ê–ì–†–£–ó–ö–ò –ò –¢–ï–°–¢–ò–†–û–í–ê–ù–ò–ï ===\")\n","# –°—Ç—Ä–∞—Ç–µ–≥–∏—è 1: –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ —É—Å—Ä–µ–¥–Ω—ë–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏\n","print(\"\\n--- –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ —É—Å—Ä–µ–¥–Ω—ë–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏ ---\")\n","loaded_model, loaded_tokenizer, loaded_config = load_bert_from_hf(f\"{BERT_REPO_NAME}_mean\", HF_TOKEN, device)\n","if loaded_model:\n","    test_text = \"–º–æ–ª–æ–∫–æ –ü—Ä–æ—Å—Ç–æ–∫–≤–∞—à–∏–Ω–æ 2.5% 1–ª\"\n","    wrapper = HFWrapper(loaded_model, loaded_tokenizer, id2label=CONFIG[\"id2label\"])\n","    doc = wrapper(test_text)\n","    entities = [(ent.start_char, ent.end_char, ent.label_) for ent in doc.ents]\n","    print(f\"–¢–µ—Å—Ç–æ–≤—ã–π —Ç–µ–∫—Å—Ç: '{test_text}'\")\n","    print(f\"–ò–∑–≤–ª–µ—á–µ–Ω–Ω—ã–µ —Å—É—â–Ω–æ—Å—Ç–∏: {entities}\")\n","    process_submission_bert(\n","        model=loaded_model,\n","        tokenizer=loaded_tokenizer,\n","        input_file=f\"{DATA_DIR}/submission.csv\",\n","        output_file=f\"{OUT_DIR}/submission_final.csv\",\n","        id2label=CONFIG[\"id2label\"]\n","    )\n","else:\n","    print(\"‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å —É—Å—Ä–µ–¥–Ω—ë–Ω–Ω—É—é –º–æ–¥–µ–ª—å\")\n"],"metadata":{"id":"4y-naEN71Mp_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","# –°—Ç—Ä–∞—Ç–µ–≥–∏—è 2: –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –∞–Ω—Å–∞–º–±–ª—è\n","print(\"\\n--- –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –∞–Ω—Å–∞–º–±–ª—è –º–æ–¥–µ–ª–µ–π ---\")\n","loaded_fold_models = [load_bert_from_hf(f\"{BERT_REPO_NAME}_fold{i+1}\", HF_TOKEN, device)[0] for i in range(1, 6)]\n","loaded_fold_models = [m for m in loaded_fold_models if m is not None]\n","if loaded_fold_models:\n","    entities = ensemble_predict(loaded_fold_models, tokenizer, test_text, is_spacy=False, id2label=CONFIG[\"id2label\"])\n","    print(f\"–¢–µ—Å—Ç–æ–≤—ã–π —Ç–µ–∫—Å—Ç: '{test_text}'\")\n","    print(f\"–ò–∑–≤–ª–µ—á–µ–Ω–Ω—ã–µ —Å—É—â–Ω–æ—Å—Ç–∏ (–∞–Ω—Å–∞–º–±–ª—å): {entities}\")\n","\n","    # –û–±—Ä–∞–±–æ—Ç–∫–∞ submission –¥–ª—è –∞–Ω—Å–∞–º–±–ª—è\n","    df = pd.read_csv(f\"{DATA_DIR}/submission.csv\", sep=';')\n","    results = []\n","    for text in df['sample']:\n","        entities = ensemble_predict(loaded_fold_models, tokenizer, text, is_spacy=False, id2label=CONFIG[\"id2label\"])\n","        results.append(entities)\n","    output_df = pd.DataFrame({'sample': df['sample'], 'annotation': results})\n","    output_df.to_csv(f\"{OUT_DIR}/submission_ensemble.csv\", sep=';', index=False)\n","    print(f\"‚úÖ –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –∞–Ω—Å–∞–º–±–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤: {OUT_DIR}/submission_ensemble.csv\")\n","else:\n","    print(\"‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å –º–æ–¥–µ–ª–∏ —Ñ–æ–ª–¥–æ–≤\")"],"metadata":{"id":"0md5goGe6WnB"},"execution_count":null,"outputs":[]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}