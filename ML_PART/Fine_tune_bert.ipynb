{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":58256,"status":"ok","timestamp":1759343738916,"user":{"displayName":"TVLG LX6500","userId":"15785277625807876484"},"user_tz":-180},"id":"0jU5kuXfijpT","outputId":"cc73f42a-0218-4442-9536-344fe2366134"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n","/content/drive/MyDrive/Colab Notebooks\n","Введи GitHub PAT токен: ··········\n","Заново склонировали репу\n","fatal: destination path 'entities-extraction-x5' already exists and is not an empty directory.\n","/content/drive/MyDrive/Colab Notebooks/entities-extraction-x5/ML_PART\n","✅ Всё готово! Рабочая папка: /content/drive/MyDrive/Colab Notebooks/entities-extraction-x5/ML_PART\n"]}],"source":["from google.colab import drive\n","import getpass, os\n","\n","# === Настройка проекта ===\n","USER = \"tokarevdr\"   # твой GitHub username\n","REPO = \"entities-extraction-x5\"            # название репозитория\n","EMAIL = \"fedorov.alexander.04@gmail.com\"    # твоя почта для git\n","NAME = \"Alexander\"           # твоё имя для git\n","# === Подключение Google Drive ===\n","drive.mount('/content/drive')\n","PROJECTS_DIR = \"/content/drive/MyDrive/Colab Notebooks\"\n","%cd $PROJECTS_DIR\n","# === GitHub авторизация ===\n","token = getpass.getpass('Введи GitHub PAT токен: ')\n","os.environ[\"GITHUB_TOKEN\"] = token\n","\n","\n","# === Проверяем: если репозиторий ещё не скачан, клонируем ===\n","if not os.path.exists(f\"{PROJECTS_DIR}/{REPO}/ML PART\"):\n","    print('Заново склонировали репу')\n","    !git clone https://{USER}:{os.environ[\"GITHUB_TOKEN\"]}@github.com/{USER}/{REPO}.git\n","# === Переходим в папку проекта ===\n","%cd {REPO}/{'ML_PART'}\n","\n","# === Настройка Git ===\n","!git config --global user.email \"{EMAIL}\"\n","!git config --global user.name \"{NAME}\"\n","!git remote set-url origin https://{USER}:{os.environ[\"GITHUB_TOKEN\"]}@github.com/{USER}/{REPO}.git\n","\n","print(\"✅ Всё готово! Рабочая папка:\", os.getcwd())\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_69974-nHwLK"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":13038,"status":"ok","timestamp":1759343752653,"user":{"displayName":"TVLG LX6500","userId":"15785277625807876484"},"user_tz":-180},"id":"43TOp5KPoqEB","outputId":"a3df1206-ede9-41af-e8e4-5175cf98cf4e"},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: torch>=2.8.0 in /usr/local/lib/python3.12/dist-packages (from -r requirements_bert.txt (line 2)) (2.8.0+cu126)\n","Requirement already satisfied: transformers>=4.35.2 in /usr/local/lib/python3.12/dist-packages (from -r requirements_bert.txt (line 3)) (4.56.1)\n","Requirement already satisfied: huggingface_hub>=0.20.3 in /usr/local/lib/python3.12/dist-packages (from -r requirements_bert.txt (line 6)) (0.35.0)\n","Requirement already satisfied: accelerate>=0.24.1 in /usr/local/lib/python3.12/dist-packages (from -r requirements_bert.txt (line 7)) (1.10.1)\n","Requirement already satisfied: tokenizers>=0.15.0 in /usr/local/lib/python3.12/dist-packages (from -r requirements_bert.txt (line 8)) (0.22.0)\n","Requirement already satisfied: datasets>=2.14.7 in /usr/local/lib/python3.12/dist-packages (from -r requirements_bert.txt (line 9)) (4.0.0)\n","Requirement already satisfied: pandas>=2.1.4 in /usr/local/lib/python3.12/dist-packages (from -r requirements_bert.txt (line 12)) (2.2.2)\n","Requirement already satisfied: numpy>=1.26.0 in /usr/local/lib/python3.12/dist-packages (from -r requirements_bert.txt (line 13)) (2.0.2)\n","Requirement already satisfied: matplotlib>=3.7.2 in /usr/local/lib/python3.12/dist-packages (from -r requirements_bert.txt (line 14)) (3.10.0)\n","Requirement already satisfied: scikit-learn>=1.3.2 in /usr/local/lib/python3.12/dist-packages (from -r requirements_bert.txt (line 15)) (1.6.1)\n","Collecting TorchCRF>=1.1.0 (from -r requirements_bert.txt (line 18))\n","  Downloading TorchCRF-1.1.0-py3-none-any.whl.metadata (2.3 kB)\n","Collecting seqeval>=1.2.2 (from -r requirements_bert.txt (line 21))\n","  Downloading seqeval-1.2.2.tar.gz (43 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.6/43.6 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch>=2.8.0->-r requirements_bert.txt (line 2)) (3.19.1)\n","Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch>=2.8.0->-r requirements_bert.txt (line 2)) (4.15.0)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=2.8.0->-r requirements_bert.txt (line 2)) (75.2.0)\n","Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=2.8.0->-r requirements_bert.txt (line 2)) (1.13.3)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch>=2.8.0->-r requirements_bert.txt (line 2)) (3.5)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.8.0->-r requirements_bert.txt (line 2)) (3.1.6)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch>=2.8.0->-r requirements_bert.txt (line 2)) (2025.3.0)\n","Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.8.0->-r requirements_bert.txt (line 2)) (12.6.77)\n","Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.8.0->-r requirements_bert.txt (line 2)) (12.6.77)\n","Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=2.8.0->-r requirements_bert.txt (line 2)) (12.6.80)\n","Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=2.8.0->-r requirements_bert.txt (line 2)) (9.10.2.21)\n","Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.8.0->-r requirements_bert.txt (line 2)) (12.6.4.1)\n","Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=2.8.0->-r requirements_bert.txt (line 2)) (11.3.0.4)\n","Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.8.0->-r requirements_bert.txt (line 2)) (10.3.7.77)\n","Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.8.0->-r requirements_bert.txt (line 2)) (11.7.1.2)\n","Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.8.0->-r requirements_bert.txt (line 2)) (12.5.4.2)\n","Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.8.0->-r requirements_bert.txt (line 2)) (0.7.1)\n","Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch>=2.8.0->-r requirements_bert.txt (line 2)) (2.27.3)\n","Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.8.0->-r requirements_bert.txt (line 2)) (12.6.77)\n","Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=2.8.0->-r requirements_bert.txt (line 2)) (12.6.85)\n","Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=2.8.0->-r requirements_bert.txt (line 2)) (1.11.1.6)\n","Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch>=2.8.0->-r requirements_bert.txt (line 2)) (3.4.0)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from transformers>=4.35.2->-r requirements_bert.txt (line 3)) (25.0)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers>=4.35.2->-r requirements_bert.txt (line 3)) (6.0.2)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers>=4.35.2->-r requirements_bert.txt (line 3)) (2024.11.6)\n","Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from transformers>=4.35.2->-r requirements_bert.txt (line 3)) (2.32.4)\n","Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers>=4.35.2->-r requirements_bert.txt (line 3)) (0.6.2)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.12/dist-packages (from transformers>=4.35.2->-r requirements_bert.txt (line 3)) (4.67.1)\n","Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub>=0.20.3->-r requirements_bert.txt (line 6)) (1.1.10)\n","Requirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (from accelerate>=0.24.1->-r requirements_bert.txt (line 7)) (5.9.5)\n","Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets>=2.14.7->-r requirements_bert.txt (line 9)) (18.1.0)\n","Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from datasets>=2.14.7->-r requirements_bert.txt (line 9)) (0.3.8)\n","Requirement already satisfied: xxhash in /usr/local/lib/python3.12/dist-packages (from datasets>=2.14.7->-r requirements_bert.txt (line 9)) (3.5.0)\n","Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.12/dist-packages (from datasets>=2.14.7->-r requirements_bert.txt (line 9)) (0.70.16)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas>=2.1.4->-r requirements_bert.txt (line 12)) (2.9.0.post0)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas>=2.1.4->-r requirements_bert.txt (line 12)) (2025.2)\n","Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas>=2.1.4->-r requirements_bert.txt (line 12)) (2025.2)\n","Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.7.2->-r requirements_bert.txt (line 14)) (1.3.3)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.7.2->-r requirements_bert.txt (line 14)) (0.12.1)\n","Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.7.2->-r requirements_bert.txt (line 14)) (4.60.0)\n","Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.7.2->-r requirements_bert.txt (line 14)) (1.4.9)\n","Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.7.2->-r requirements_bert.txt (line 14)) (11.3.0)\n","Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.7.2->-r requirements_bert.txt (line 14)) (3.2.4)\n","Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn>=1.3.2->-r requirements_bert.txt (line 15)) (1.16.2)\n","Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn>=1.3.2->-r requirements_bert.txt (line 15)) (1.5.2)\n","Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn>=1.3.2->-r requirements_bert.txt (line 15)) (3.6.0)\n","Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.14.7->-r requirements_bert.txt (line 9)) (3.12.15)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas>=2.1.4->-r requirements_bert.txt (line 12)) (1.17.0)\n","Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->transformers>=4.35.2->-r requirements_bert.txt (line 3)) (3.4.3)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->transformers>=4.35.2->-r requirements_bert.txt (line 3)) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->transformers>=4.35.2->-r requirements_bert.txt (line 3)) (2.5.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->transformers>=4.35.2->-r requirements_bert.txt (line 3)) (2025.8.3)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=2.8.0->-r requirements_bert.txt (line 2)) (1.3.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=2.8.0->-r requirements_bert.txt (line 2)) (3.0.2)\n","Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.14.7->-r requirements_bert.txt (line 9)) (2.6.1)\n","Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.14.7->-r requirements_bert.txt (line 9)) (1.4.0)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.14.7->-r requirements_bert.txt (line 9)) (25.3.0)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.14.7->-r requirements_bert.txt (line 9)) (1.7.0)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.14.7->-r requirements_bert.txt (line 9)) (6.6.4)\n","Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.14.7->-r requirements_bert.txt (line 9)) (0.3.2)\n","Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.14.7->-r requirements_bert.txt (line 9)) (1.20.1)\n","Downloading TorchCRF-1.1.0-py3-none-any.whl (5.2 kB)\n","Building wheels for collected packages: seqeval\n","  Building wheel for seqeval (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for seqeval: filename=seqeval-1.2.2-py3-none-any.whl size=16162 sha256=ea1669fa1edaa8a26beeaff577463c17a29182ea96a6f4bb1f53df04ee984119\n","  Stored in directory: /root/.cache/pip/wheels/5f/b8/73/0b2c1a76b701a677653dd79ece07cfabd7457989dbfbdcd8d7\n","Successfully built seqeval\n","Installing collected packages: seqeval, TorchCRF\n","Successfully installed TorchCRF-1.1.0 seqeval-1.2.2\n"]}],"source":["# Установка зависимостей\n","!pip install -r requirements_bert.txt\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":14158,"status":"ok","timestamp":1759343766813,"user":{"displayName":"TVLG LX6500","userId":"15785277625807876484"},"user_tz":-180},"id":"WKM-_g4rH0SC","outputId":"7d64de6e-523b-4f58-f38d-640348c99fe6"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting onnxruntime\n","  Downloading onnxruntime-1.23.0-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (4.9 kB)\n","Collecting coloredlogs (from onnxruntime)\n","  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl.metadata (12 kB)\n","Requirement already satisfied: flatbuffers in /usr/local/lib/python3.12/dist-packages (from onnxruntime) (25.2.10)\n","Requirement already satisfied: numpy>=1.21.6 in /usr/local/lib/python3.12/dist-packages (from onnxruntime) (2.0.2)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from onnxruntime) (25.0)\n","Requirement already satisfied: protobuf in /usr/local/lib/python3.12/dist-packages (from onnxruntime) (5.29.5)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.12/dist-packages (from onnxruntime) (1.13.3)\n","Collecting humanfriendly>=9.1 (from coloredlogs->onnxruntime)\n","  Downloading humanfriendly-10.0-py2.py3-none-any.whl.metadata (9.2 kB)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy->onnxruntime) (1.3.0)\n","Downloading onnxruntime-1.23.0-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (17.3 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.3/17.3 MB\u001b[0m \u001b[31m64.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: humanfriendly, coloredlogs, onnxruntime\n","Successfully installed coloredlogs-15.0.1 humanfriendly-10.0 onnxruntime-1.23.0\n"]}],"source":["! pip install --upgrade onnxruntime"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":27604,"status":"ok","timestamp":1759343794435,"user":{"displayName":"TVLG LX6500","userId":"15785277625807876484"},"user_tz":-180},"id":"cU-8Ca9forrD","outputId":"43950f1d-5c34-449a-a6b9-79d8ff770508"},"outputs":[{"output_type":"stream","name":"stdout","text":["✅ Transformers успешно импортированы\n","✅ TorchCRF успешно импортирован\n"]}],"source":["from google.colab import drive\n","import getpass, os, json, random, time\n","import numpy as np\n","import pandas as pd\n","import torch\n","import matplotlib.pyplot as plt\n","from sklearn.model_selection import train_test_split, KFold\n","from torch.utils.data import Dataset, DataLoader\n","from torch.optim import AdamW\n","\n","# Импорты transformers с обработкой ошибок\n","try:\n","    from transformers import AutoTokenizer, AutoModelForTokenClassification, get_scheduler\n","    print(\"✅ Transformers успешно импортированы\")\n","except ImportError as e:\n","    print(f\"❌ Ошибка импорта transformers: {e}\")\n","    !pip install transformers==4.35.2\n","    from transformers import AutoTokenizer, AutoModelForTokenClassification, get_scheduler\n","\n","try:\n","    from TorchCRF import CRF\n","    print(\"✅ TorchCRF успешно импортирован\")\n","except ImportError as e:\n","    print(f\"❌ Ошибка импорта TorchCRF: {e}\")\n","    !pip install TorchCRF==1.1.0\n","    from TorchCRF import CRF\n","\n","import ast\n","import traceback\n","from module import calculate_ner_metrics, calculate_macro_f1, evaluate_model, check_repo_exists,\\\n"," process_submission, parse_span_str, merge_prefixed_char_spans, tokenize_and_align_labels, \\\n"," token_labels_to_char_spans, build_label_maps_from_examples, HFWrapper, setup_hf_login, \\\n"," save_spacy_to_hf, list_my_repos, load_spacy_from_hf, NERModelWithCRF, save_bert_to_hf, load_bert_from_hf, process_submission_bert, ensemble_predict\n","from torch.nn.utils.rnn import pad_sequence"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bO4KcUNr0ASv"},"outputs":[],"source":["# --- Основные пути для сохранения результатов ---\n","WHERE_DATA = 'cleared_data'\n","BASE_MODEL_NAME = \"DeepPavlov/rubert-base-cased\"\n","OUT_DIR = f\"OUTPUT/{WHERE_DATA}/{BASE_MODEL_NAME}\"\n","os.makedirs(OUT_DIR, exist_ok=True)       # папка для сохранения всех файлов\n","FINAL_METRICS_PATH = f\"{OUT_DIR}/final_training_metrics_per_epoch.csv\"\n","MODEL_PATH = f'MODELS/{WHERE_DATA}/{BASE_MODEL_NAME}'\n","os.makedirs(MODEL_PATH, exist_ok=True)\n","DATA_DIR = f'data/{WHERE_DATA}/'\n","PATIENCE = 3      # количество эпох без улучшения F1 до остановки\n","SEED = 42\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":18280,"status":"ok","timestamp":1759343813513,"user":{"displayName":"TVLG LX6500","userId":"15785277625807876484"},"user_tz":-180},"id":"0KGUxpTbJQSH","outputId":"1ddc3861-2000-40ae-8896-8fcec7c38e34"},"outputs":[{"output_type":"stream","name":"stdout","text":["Введи HFT токен: ··········\n","✅ Авторизация HF настроена\n"]},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":7}],"source":["\n","# Hugging Face настройки\n","HF_TOKEN= getpass.getpass('Введи HFT токен: ')\n","HF_USERNAME = \"alexflex04\"\n","BERT_REPO_NAME = f\"{HF_USERNAME}/NER_{WHERE_DATA}_bert\"\n","\n","setup_hf_login(HF_TOKEN)"]},{"cell_type":"code","source":["CONFIG = {\n","    \"model_checkpoint\": BASE_MODEL_NAME,\n","    \"num_epochs\": 10,\n","    \"batch_size\": 64,\n","    \"learning_rate\": 2e-5,\n","    \"weight_decay\": 0.01,\n","    \"patience\": PATIENCE,\n","    \"max_length\": 128,\n","    \"label_list\": [\"O\", \"B-TYPE\", \"I-TYPE\", \"B-BRAND\", \"I-BRAND\", \"B-VOLUME\", \"I-VOLUME\", \"B-PERCENT\", \"I-PERCENT\"],\n","    \"id2label\": {i: label for i, label in enumerate([\"O\", \"B-TYPE\", \"I-TYPE\", \"B-BRAND\", \"I-BRAND\", \"B-VOLUME\", \"I-VOLUME\", \"B-PERCENT\", \"I-PERCENT\"])},\n","    \"label2id\": {label: i for i, label in enumerate([\"O\", \"B-TYPE\", \"I-TYPE\", \"B-BRAND\", \"I-BRAND\", \"B-VOLUME\", \"I-VOLUME\", \"B-PERCENT\", \"I-PERCENT\"])},\n","\n","}"],"metadata":{"id":"IHotQkii7_BW"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"t_VWEzvBF9nH"},"outputs":[],"source":["os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mFL3k8dl0FOh"},"outputs":[],"source":["random.seed(SEED)\n","np.random.seed(SEED)\n","torch.manual_seed(SEED)\n","torch.cuda.manual_seed_all(SEED)\n","torch.backends.cudnn.deterministic = True\n","torch.backends.cudnn.benchmark = False"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":189,"status":"ok","timestamp":1759343818139,"user":{"displayName":"TVLG LX6500","userId":"15785277625807876484"},"user_tz":-180},"id":"jgkUd-P9ov1X","outputId":"55d4315b-4cd6-4067-c7ea-a9f30ef903a7"},"outputs":[{"output_type":"stream","name":"stdout","text":["Используемое устройство: cuda\n"]}],"source":["device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","print(f\"Используемое устройство: {device}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4j7jeRhb0Zfz"},"outputs":[],"source":["# Загрузка данных\n","train_split = pd.read_csv(f\"{DATA_DIR}train.csv\")\n","valid_data = pd.read_csv(f\"{DATA_DIR}val.csv\")\n","\n","def parse_row_to_example(row):\n","    try:\n","        ann = ast.literal_eval(row['annotation'])\n","    except Exception:\n","        ann = []\n","    return (row['sample'],  ann)\n","\n","train_data = [parse_row_to_example(row) for _, row in train_split.iterrows()]\n","valid_data = [parse_row_to_example(row) for _, row in valid_data.iterrows()]\n","\n","from transformers import AutoTokenizer\n","\n","tokenizer = AutoTokenizer.from_pretrained(\"DeepPavlov/rubert-base-cased\", use_fast=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KaFuwbUZzP9n"},"outputs":[],"source":["# examples = [\n","#     (\"яйцо куриное\", [(0, 4, 'B-TYPE'), (5, 12, 'I-TYPE')]),\n","#     (\"яйцо куриное 30шт\", [(0,4,'B-TYPE'), (5,12,'I-TYPE'), (13,17,'B-VOLUME')]),\n","#     (\"сок 0.2 л 10%\", [(0,3,'B-TYPE'), (4,7,'B-VOLUME'), (8,9,'I-VOLUME'), (10,13,'B-PERCENT')]),\n","#     (\"масло сливочное 72% 250 г President\", [(0,5,'B-TYPE'), (6,15,'I-TYPE'), (16,19,'B-PERCENT'), (20,23,'B-VOLUME'), (24,25,'I-VOLUME'), (26,35,'B-BRAND')]),\n","#     (\"пиво Baltika 4.8% 0.5 л\", [(0,4,'B-TYPE'), (5,12,'B-BRAND'), (13,17,'B-PERCENT'), (18,21,'B-VOLUME'), (22,23,'I-VOLUME')]),\n","#     (\"global village летняя ягода\", [(0,6,'B-BRAND'), (7,14,'I-BRAND'), (15,21,'B-TYPE'), (22,27,'I-TYPE')]),\n","#     (\"arkhangel'skkhleb багет\", [(0,17,'B-BRAND'), (18,23,'B-TYPE')]),\n","#     (\"aunfed\", [(0,6,'O')]),\n","#     (\"bunk club\", [(0,4,'O'), (5,9,'O')]),\n","# ]\n","\n","# # Run tests\n","# for text, entities in examples:\n","#     print(f\"TEXT:{text} Entities:{entities}\")\n","#     parsed = entities  # if you had strings: parse_span_str(...)\n","#     enc = tokenize_and_align_labels(text, parsed, tokenizer, add_special_tokens=True)\n","#     tokens = enc['tokens']; offsets = enc['offsets']; tlabels = enc['token_labels']\n","#     print(\"Токены:\", tokens)\n","#     print(\"OFFSETS:\", offsets)\n","#     print(\"TOKEN LABELS:\", tlabels)\n","#     recovered = token_labels_to_char_spans(offsets, tlabels)\n","#     print(\"RECOVERED CHAR-SPANS:\", recovered)\n","#     merged = merge_prefixed_char_spans(parsed)\n","#     # prepare expected in recovered-format: B-<BASE> for entities, 'O' for O\n","#     expected = []\n","#     for s,e,lab in merged:\n","#         if lab == 'O':\n","#             expected.append((s,e,'O'))\n","#         else:\n","#             expected.append((s,e,'B-' + lab))\n","#     print(\"EXPECTED (merged):\", expected)\n","#     ok = (recovered == expected)\n","#     print(\"ROUND-TRIP OK:\", ok)\n","#     if not ok:\n","#         print(\"NOTE: mismatches can happen if tokenizer splits differently; inspect offsets and labels.\")\n","#     print(\"-\" * 60)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ViwlaKNrFIkC"},"outputs":[],"source":["class NERDataset(Dataset):\n","    def __init__(self, data, tokenizer, label2id):\n","        self.data = data\n","        self.tokenizer = tokenizer\n","        self.label2id = label2id\n","\n","    def __len__(self):\n","        return len(self.data)\n","\n","    def __getitem__(self, idx):\n","        text, annotations = self.data[idx]\n","        entities = annotations\n","\n","        # Добавляем параметры truncation и max_length в вызов tokenize_and_align_labels\n","        # (предполагая, что функция tokenize_and_align_labels принимает их; если нет, добавьте в определение функции)\n","        tokenized = tokenize_and_align_labels(\n","            text,\n","            entities,\n","            self.tokenizer,\n","            add_special_tokens=True,\n","            truncation=True,\n","            max_length=CONFIG[\"max_length\"]  # Используем CONFIG из глобального контекста\n","        )\n","\n","        # Преобразуем текстовые метки в числовые ID\n","        labels = [self.label2id[label] for label in tokenized['token_labels']]\n","\n","        # Преобразуем в тензоры\n","        input_ids = torch.tensor(tokenized['input_ids'])\n","        attention_mask = torch.tensor([1] * len(tokenized['input_ids']))\n","        labels = torch.tensor(labels)\n","\n","        # Устанавливаем -100 для [CLS] (index 0) и [SEP] (last index), чтобы игнорировать их в лоссе\n","        labels[0] = -100\n","        labels[-1] = -100\n","\n","        return {\n","            'input_ids': input_ids,\n","            'attention_mask': attention_mask,\n","            'labels': labels\n","        }\n","\n","def collate_fn(batch):\n","    input_ids = [item[\"input_ids\"] for item in batch]\n","    attention_mask = [item[\"attention_mask\"] for item in batch]\n","    labels = [item[\"labels\"] for item in batch]\n","\n","    input_ids = pad_sequence(input_ids, batch_first=True, padding_value=0)\n","    attention_mask = pad_sequence(attention_mask, batch_first=True, padding_value=0)\n","    labels = pad_sequence(labels, batch_first=True, padding_value=-100)  # -100 для игнорирования в loss\n","\n","    return {\n","        \"input_ids\": input_ids,\n","        \"attention_mask\": attention_mask,\n","        \"labels\": labels\n","    }\n","\n","\n","\n","def evaluate_model(model, eval_data, tokenizer, id2label):\n","    entity_pairs = []\n","    model.eval()\n","    # print(eval_data)\n","\n","    for text, entities in eval_data:\n","\n","        # print(text, entities)\n","\n","        tokenized = tokenizer(\n","            [text],\n","            padding=True,\n","            truncation=True,\n","            max_length=512,\n","            return_tensors=\"pt\",\n","            return_offsets_mapping=True\n","        )\n","\n","        device = next(model.parameters()).device\n","        input_ids = tokenized[\"input_ids\"].to(device)\n","        attention_mask = tokenized[\"attention_mask\"].to(device)\n","\n","        with torch.no_grad():\n","            # Получаем emissions\n","            emissions = model.get_emissions(input_ids, attention_mask)\n","            # Используем argmax для предсказания тегов\n","            pred = torch.argmax(emissions, dim=-1)[0]\n","\n","        bio_labels = [id2label[p.item()] for p in pred]\n","        offsets = tokenized[\"offset_mapping\"][0].tolist()\n","        pred_entities = token_labels_to_char_spans(offsets, bio_labels)\n","        true_entities = entities\n","        entity_pairs.append((true_entities, pred_entities))\n","\n","    macro_f1, f1_type, f1_brand, f1_volume, f1_percent = calculate_macro_f1(entity_pairs)\n","    return {\n","        'f1_macro': macro_f1,\n","        'f1_TYPE': f1_type,\n","        'f1_BRAND': f1_brand,\n","        'f1_VOLUME': f1_volume,\n","        'f1_PERCENT': f1_percent\n","    }"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1786,"status":"ok","timestamp":1759347007573,"user":{"displayName":"TVLG LX6500","userId":"15785277625807876484"},"user_tz":-180},"id":"92aearUZ0hEQ","outputId":"9d346d45-51c4-4645-ad9c-ca8c66c57dab"},"outputs":[{"output_type":"stream","name":"stdout","text":["=== ИНИЦИАЛИЗАЦИЯ МОДЕЛИ ===\n"]},{"output_type":"stream","name":"stderr","text":["Some weights of BertForTokenClassification were not initialized from the model checkpoint at DeepPavlov/rubert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"output_type":"stream","name":"stdout","text":["✅ Модель и данные успешно подготовлены!\n","Размер обучающей выборки: 21826\n"]}],"source":["print(\"=== ИНИЦИАЛИЗАЦИЯ МОДЕЛИ ===\")\n","model = NERModelWithCRF(len(CONFIG[\"label_list\"])).to(device)\n","optimizer = AdamW(model.parameters(), lr=CONFIG[\"learning_rate\"], weight_decay=CONFIG[\"weight_decay\"])\n","num_training_steps = CONFIG[\"num_epochs\"] * len(train_data) // CONFIG[\"batch_size\"]\n","scheduler = get_scheduler(\"linear\", optimizer=optimizer, num_warmup_steps=0, num_training_steps=num_training_steps)\n","\n","train_dataset = NERDataset(train_data, tokenizer, label2id=CONFIG['label2id'])\n","train_loader = DataLoader(train_dataset, batch_size=CONFIG[\"batch_size\"], shuffle=True, collate_fn=collate_fn)\n","\n","\n","metrics_df = pd.DataFrame(columns=['epoch', 'loss', 'f1_macro', 'f1_TYPE', 'f1_BRAND', 'f1_VOLUME', 'f1_PERCENT'])\n","best_f1 = 0\n","patience_counter = 0\n","best_epoch = 0\n","\n","print(\"✅ Модель и данные успешно подготовлены!\")\n","print(f\"Размер обучающей выборки: {len(train_dataset)}\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4,"status":"ok","timestamp":1759345723922,"user":{"displayName":"TVLG LX6500","userId":"15785277625807876484"},"user_tz":-180},"id":"-DUbDrZMTLlP","outputId":"8f36f82c-b48f-4ca0-8417-3562b2fe12cf"},"outputs":[{"output_type":"stream","name":"stdout","text":["NERModelWithCRF(\n","  (bert): BertForTokenClassification(\n","    (bert): BertModel(\n","      (embeddings): BertEmbeddings(\n","        (word_embeddings): Embedding(119547, 768, padding_idx=0)\n","        (position_embeddings): Embedding(512, 768)\n","        (token_type_embeddings): Embedding(2, 768)\n","        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","        (dropout): Dropout(p=0.1, inplace=False)\n","      )\n","      (encoder): BertEncoder(\n","        (layer): ModuleList(\n","          (0-11): 12 x BertLayer(\n","            (attention): BertAttention(\n","              (self): BertSdpaSelfAttention(\n","                (query): Linear(in_features=768, out_features=768, bias=True)\n","                (key): Linear(in_features=768, out_features=768, bias=True)\n","                (value): Linear(in_features=768, out_features=768, bias=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (output): BertSelfOutput(\n","                (dense): Linear(in_features=768, out_features=768, bias=True)\n","                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","            )\n","            (intermediate): BertIntermediate(\n","              (dense): Linear(in_features=768, out_features=3072, bias=True)\n","              (intermediate_act_fn): GELUActivation()\n","            )\n","            (output): BertOutput(\n","              (dense): Linear(in_features=3072, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","        )\n","      )\n","    )\n","    (dropout): Dropout(p=0.1, inplace=False)\n","    (classifier): Linear(in_features=768, out_features=9, bias=True)\n","  )\n","  (crf): CRF()\n",")\n","True\n"]}],"source":["print(model)  # посмотрите архитектуру\n","print(hasattr(model, 'crf'))  # проверьте наличие CRF слоя\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8IyWSwnO0jQ4","outputId":"54fc405f-8258-4a0e-d894-9f8345148339"},"outputs":[{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["\n","=== НАЧАЛО SCREENING ОБУЧЕНИЯ ===\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["/tmp/ipython-input-1198690905.py:30: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n","  metrics_df = pd.concat([metrics_df, pd.DataFrame([metrics_row])], ignore_index=True)\n"]},{"output_type":"stream","name":"stdout","text":["Эпоха 1   | Loss: 1.4249 | F1-macro: 0.5000 | F1-TYPE: 0.7242 | F1-BRAND: 0.7959 | F1-VOLUME: 0.0800 | F1-PERCENT: 0.4000\n","Эпоха 2   | Loss: 0.6208 | F1-macro: 0.5676 | F1-TYPE: 0.7265 | F1-BRAND: 0.8254 | F1-VOLUME: 0.1852 | F1-PERCENT: 0.5333\n","Эпоха 3   | Loss: 0.4289 | F1-macro: 0.5870 | F1-TYPE: 0.7322 | F1-BRAND: 0.8235 | F1-VOLUME: 0.2041 | F1-PERCENT: 0.5882\n"]}],"source":["print(\"\\n=== НАЧАЛО SCREENING ОБУЧЕНИЯ ===\")\n","try:\n","    for epoch in range(CONFIG[\"num_epochs\"]):\n","        model.train()\n","        total_loss = 0\n","        for batch in train_loader:\n","            input_ids = batch[\"input_ids\"].to(device)\n","            attention_mask = batch[\"attention_mask\"].to(device)\n","            labels = batch[\"labels\"].to(device)\n","            loss = model(input_ids, attention_mask, labels)\n","\n","            # Усредняем лосс по батчу и затем получаем скаляр\n","            loss_mean = loss.mean()  # Добавляем эту строку\n","            total_loss += loss_mean.item()  # Используем усредненное значение\n","\n","            optimizer.zero_grad()\n","            loss_mean.backward()  # Используем усредненное значение для обратного распространения\n","            optimizer.step()\n","            scheduler.step()\n","\n","        avg_loss = total_loss / len(train_loader)\n","        eval_metrics = evaluate_model(model, valid_data, tokenizer, id2label=CONFIG['id2label'])\n","        current_f1 = eval_metrics[\"f1_macro\"]\n","\n","        metrics_row = {\n","            'epoch': epoch + 1,\n","            'loss': avg_loss,\n","            **eval_metrics\n","        }\n","        metrics_df = pd.concat([metrics_df, pd.DataFrame([metrics_row])], ignore_index=True)\n","\n","        print(f'Эпоха {epoch + 1:<3} | Loss: {avg_loss:.4f} | '\n","              f'F1-macro: {current_f1:.4f} | '\n","              f'F1-TYPE: {eval_metrics[\"f1_TYPE\"]:.4f} | '\n","              f'F1-BRAND: {eval_metrics[\"f1_BRAND\"]:.4f} | '\n","              f'F1-VOLUME: {eval_metrics[\"f1_VOLUME\"]:.4f} | '\n","              f'F1-PERCENT: {eval_metrics[\"f1_PERCENT\"]:.4f}')\n","\n","        if current_f1 > best_f1:\n","            best_f1 = current_f1\n","            best_epoch = epoch + 1\n","            patience_counter = 0\n","        else:\n","            patience_counter += 1\n","            print(f\"⏳ Patience: {patience_counter}/{PATIENCE}\")\n","            if patience_counter >= PATIENCE:\n","                print(f\"\\n🛑 Ранняя остановка на эпохе {epoch + 1}\")\n","                print(f\"Лучший F1-macro: {best_f1:.4f} достигнут на эпохе {best_epoch}\")\n","                break\n","\n","except Exception as e:\n","    print(f'💥 Критическая ошибка: {str(e)}')\n","    print(traceback.format_exc())\n","\n","finally:\n","    # Сохранение screening модели на HF\n","    print(f\"\\n💾 Сохранение screening модели на HF: {BERT_REPO_NAME+'_screening'}\")\n","    success = save_bert_to_hf(model, tokenizer, CONFIG, BERT_REPO_NAME+'_screening', HF_TOKEN)\n","\n","    if success:\n","        print(f\"🎉 BERT screening модель успешно сохранена на HF: {BERT_REPO_NAME+'_screening'}\")\n","    else:\n","        print(\"❌ Не удалось сохранить BERT screening модель на HF\")\n","\n","    # Локальное сохранение\n","    # torch.save(model.state_dict(), f\"{MODEL_PATH}/model_screening.pt\")\n","    metrics_df.to_csv(f'{OUT_DIR}/screening_metrics.csv', index=False)\n","    print(\"💾Метрики сохранены локально\")\n","\n","print(\"\\n\" + \"=\"*80)\n","print(\"ИТОГОВЫЕ РЕЗУЛЬТАТЫ SCREENING:\")\n","print(\"=\"*80)\n","print(f\"Лучший F1-macro: {best_f1:.4f} на эпохе {best_epoch}\")\n","print(f\"Всего эпох выполнено: {len(metrics_df)}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"a8NbgDCk0mEW"},"outputs":[],"source":["# Визуализация\n","plt.figure(figsize=(15, 10))\n","plt.subplot(2, 1, 1)\n","plt.plot(metrics_df['epoch'], metrics_df['loss'], 'b-', linewidth=2, label='Loss')\n","plt.axvline(x=best_epoch, color='r', linestyle='--', alpha=0.7, label=f'Лучшая эпоха ({best_epoch})')\n","plt.xlabel('Эпоха')\n","plt.ylabel('Loss')\n","plt.title('Loss по эпохам')\n","plt.legend()\n","plt.grid(True, alpha=0.3)\n","\n","plt.subplot(2, 1, 2)\n","plt.plot(metrics_df['epoch'], metrics_df['f1_macro'], 'r-', linewidth=3, label='F1-macro')\n","plt.plot(metrics_df['epoch'], metrics_df['f1_TYPE'], 'g--', label='F1-TYPE')\n","plt.plot(metrics_df['epoch'], metrics_df['f1_BRAND'], 'b--', label='F1-BRAND')\n","plt.plot(metrics_df['epoch'], metrics_df['f1_VOLUME'], 'y--', label='F1-VOLUME')\n","plt.plot(metrics_df['epoch'], metrics_df['f1_PERCENT'], 'c--', label='F1-PERCENT')\n","plt.axvline(x=best_epoch, color='r', linestyle='--', alpha=0.7, label=f'Лучшая эпоха ({best_epoch})')\n","plt.xlabel('Эпоха')\n","plt.ylabel('F1 Score')\n","plt.title('F1 Scores по эпохам')\n","plt.legend()\n","plt.grid(True, alpha=0.3)\n","\n","plt.tight_layout()\n","plt.savefig(f\"{OUT_DIR}/screening_metrics.png\", dpi=300, bbox_inches='tight')\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"oVXcgPQDKOoV"},"outputs":[],"source":["print(\"\\n=== ПРОВЕРКА ЗАГРУЗКИ МОДЕЛИ ===\")\n","loaded_model, loaded_tokenizer, loaded_config = load_bert_from_hf(BERT_REPO_NAME+'_screening', HF_TOKEN, device)\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ivHG4pUMDQdc"},"outputs":[],"source":["if loaded_model:\n","    print(\"✅ Модель успешно загружена с HF!\")\n","    test_text = \"молоко Простоквашино 2.5% 1л\"\n","    from module import HFWrapper\n","    wrapper = HFWrapper(loaded_model, loaded_tokenizer, id2label=CONFIG['id2label'])\n","    doc = wrapper(test_text)\n","    entities = [(ent.start_char, ent.end_char, ent.label_) for ent in doc.ents]\n","    print(f\"Тестовый текст: '{test_text}'\")\n","    print(f\"Извлеченные сущности: {entities}\")\n","\n","    # Обработка submission файла\n","    print(f\"\\n=== ОБРАБОТКА SUBMISSION ФАЙЛА ===\")\n","    process_submission_bert(\n","        model=loaded_model,\n","        tokenizer=loaded_tokenizer,\n","        input_file=os.getcwd()+'/data/submission.csv',\n","        output_file=f\"{OUT_DIR}/submission_screening.csv\",\n","        id2label=CONFIG['id2label']\n","    )\n","else:\n","    print(\"❌ Не удалось загрузить модель для тестирования\")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":8,"status":"ok","timestamp":1759274683830,"user":{"displayName":"Александр Федоров","userId":"11558309455709398811"},"user_tz":-180},"id":"cM7CtsGwKO5w","outputId":"de8afc09-dc7e-45da-dd04-5ef2b2124e4f"},"outputs":[{"name":"stdout","output_type":"stream","text":["[]\n"]}],"source":["print(doc.ents)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6vKo5oFn0s2V"},"outputs":[],"source":["# Ячейка 2: Подбор гиперпараметров (Tuning) с grid search\n","# PARAM_GRID = {\n","#     \"learning_rate\": [1e-5, 2e-5, 3e-5],\n","#     \"batch_size\": [32, 64],\n","#     \"epochs\": [10, 20],\n","#     \"weight_decay\": [0.01, 0.1]\n","# }\n","\n","\n","PARAM_GRID = {\n","    \"learning_rate\": [ 3e-5],\n","    \"batch_size\": [64],\n","    \"epochs\": [3],\n","    \"weight_decay\": [ 0.1]\n","}\n","grid_results = []\n","\n","for lr in PARAM_GRID[\"learning_rate\"]:\n","    for bsz in PARAM_GRID[\"batch_size\"]:\n","        for max_ep in PARAM_GRID[\"epochs\"]:\n","            for wd in PARAM_GRID[\"weight_decay\"]:\n","                combo = {\"learning_rate\": lr, \"batch_size\": bsz, \"epochs\": max_ep, \"weight_decay\": wd}\n","                print(f\"\\n=== Tuning combo: learning_rate={lr}, batch_size={bsz}, epochs={max_ep}, weight_decay={wd} ===\")\n","\n","                model = NERModelWithCRF(len(CONFIG[\"label_list\"])).to(device)\n","                optimizer = AdamW(model.parameters(), lr=lr, weight_decay=wd)\n","                num_training_steps = max_ep * len(train_data) // bsz\n","                scheduler = get_scheduler(\"linear\", optimizer=optimizer, num_warmup_steps=0, num_training_steps=num_training_steps)\n","\n","                train_loader = DataLoader(train_dataset, batch_size=bsz, shuffle=True)\n","\n","                patience_counter, best_f1, best_metrics = 0, 0.0, None\n","                for epoch in range(1, max_ep + 1):\n","                    model.train()\n","                    total_loss = 0\n","                    for batch in train_loader:\n","                        input_ids = batch[\"input_ids\"].to(device)\n","                        attention_mask = batch[\"attention_mask\"].to(device)\n","                        labels = batch[\"labels\"].to(device)\n","                        loss = model(input_ids, attention_mask, labels)\n","                        loss_mean = loss.mean()\n","                        total_loss += loss_mean.item()\n","                        optimizer.zero_grad()\n","                        loss_mean.backward()\n","                        optimizer.step()\n","                        scheduler.step()\n","\n","                    avg_loss = total_loss / len(train_loader)\n","                    metrics = evaluate_model(model, valid_data, tokenizer)\n","                    metrics[\"epoch\"] = int(epoch)\n","                    metrics[\"loss\"] = float(avg_loss)\n","                    current_f1 = metrics[\"f1_macro\"]\n","\n","                    print(f\"Ep {epoch} | Loss: {metrics['loss']:.4f} | F1-macro: {current_f1:.4f}\")\n","\n","                    if current_f1 > best_f1:\n","                        best_f1 = current_f1\n","                        best_metrics = metrics\n","                        patience_counter = 0\n","                    else:\n","                        patience_counter += 1\n","                        if patience_counter >= PATIENCE:\n","                            break\n","\n","                combo[\"best_f1_macro\"] = best_f1\n","                combo[\"best_metrics\"] = best_metrics\n","                grid_results.append(combo)\n","\n","# Выбор лучших параметров\n","best_combo = max(grid_results, key=lambda x: x[\"best_f1_macro\"])\n","print(\"\\nBest tuning params:\", best_combo)\n","\n","# Сохранение результатов\n","pd.DataFrame(grid_results).to_csv(f\"{OUT_DIR}/tuning_summary.csv\", index=False)\n","with open(f\"{OUT_DIR}/tuning_detailed.json\", \"w\", encoding=\"utf-8\") as f:\n","    json.dump(grid_results, f, ensure_ascii=False, indent=2)\n","with open(f\"{OUT_DIR}/best_combo.json\", \"w\", encoding=\"utf-8\") as f:\n","    json.dump({k: v for k, v in best_combo.items() if k != \"best_metrics\"}, f, ensure_ascii=False, indent=2)\n","print(\"💾 Tuning результаты и best_combo сохранены\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gU0AhvFT0tZq"},"outputs":[],"source":["# Ячейка 3: Кросс-валидация (CV) с лучшими параметрами\n","with open(f\"{OUT_DIR}/best_combo.json\", \"r\", encoding=\"utf-8\") as f:\n","    best_combo = json.load(f)\n","\n","best_lr = best_combo[\"learning_rate\"]\n","best_bsz = best_combo[\"batch_size\"]\n","best_max_ep = best_combo[\"epochs\"]\n","best_wd = best_combo[\"weight_decay\"]\n","\n","kf = KFold(n_splits=5, shuffle=True, random_state=SEED)\n","cv_results = []\n","fold_best_f1s = []\n","\n","for fold, (tr_idx, val_idx) in enumerate(kf.split(train_data), 1):\n","    print(f\"\\n=== CV Fold {fold} ===\")\n","    fold_train = [train_data[i] for i in tr_idx]\n","    fold_valid = [train_data[i] for i in val_idx]\n","\n","    fold_train_dataset = NERDataset(fold_train, tokenizer)\n","    fold_train_loader = DataLoader(fold_train_dataset, batch_size=best_bsz, shuffle=True)\n","\n","    model = NERModelWithCRF(len(CONFIG[\"label_list\"])).to(device)\n","    optimizer = AdamW(model.parameters(), lr=best_lr, weight_decay=best_wd)\n","    num_training_steps = best_max_ep * len(fold_train) // best_bsz\n","    scheduler = get_scheduler(\"linear\", optimizer=optimizer, num_warmup_steps=0, num_training_steps=num_training_steps)\n","\n","    patience_counter, best_f1, best_metrics = 0, 0.0, None\n","    for epoch in range(1, best_max_ep + 1):\n","        model.train()\n","        total_loss = 0\n","        for batch in fold_train_loader:\n","            input_ids = batch[\"input_ids\"].to(device)\n","            attention_mask = batch[\"attention_mask\"].to(device)\n","            labels = batch[\"labels\"].to(device)\n","            loss = model(input_ids, attention_mask, labels)\n","            loss_mean = loss.mean()\n","            total_loss += loss_mean.item()\n","            optimizer.zero_grad()\n","            loss_mean.backward()\n","            optimizer.step()\n","            scheduler.step()\n","\n","        avg_loss = total_loss / len(fold_train_loader)\n","        metrics = evaluate_model(model, fold_valid, tokenizer)\n","        metrics[\"epoch\"] = epoch\n","        metrics[\"loss\"] = avg_loss\n","        current_f1 = metrics[\"f1_macro\"]\n","\n","        print(f\"Fold {fold} Ep {epoch} | Loss: {metrics['loss']:.4f} | F1-macro: {current_f1:.4f}\")\n","\n","        if current_f1 > best_f1:\n","            best_f1 = current_f1\n","            best_metrics = metrics\n","            patience_counter = 0\n","        else:\n","            patience_counter += 1\n","            if patience_counter >= PATIENCE:\n","                break\n","\n","    cv_results.append({\"fold\": fold, \"best_f1_macro\": best_f1, \"best_metrics\": best_metrics})\n","    fold_best_f1s.append(best_f1)\n","\n","mean_f1 = np.mean(fold_best_f1s)\n","std_f1 = np.std(fold_best_f1s)\n","print(f\"\\nCV Results: Mean F1_macro = {mean_f1:.4f} ± {std_f1:.4f}\")\n","\n","# Сохранение результатов CV\n","pd.DataFrame(cv_results).to_csv(f\"{OUT_DIR}/cv_summary.csv\", index=False)\n","with open(f\"{OUT_DIR}/cv_detailed.json\", \"w\", encoding=\"utf-8\") as f:\n","    json.dump(cv_results, f, ensure_ascii=False, indent=2)\n","print(\"💾 CV результаты сохранены\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Wz7v_fJW0ycg"},"outputs":[],"source":["# === Стратегия 1: Усреднённая модель ===\n","print(\"\\n=== Стратегия 1: Финальное обучение (усреднённая модель) ===\")\n","train_val = train_data + valid_data\n","train_val_dataset = NERDataset(train_val, tokenizer)\n","train_val_loader = DataLoader(train_val_dataset, batch_size=best_bsz, shuffle=True)\n","\n","model = NERModelWithCRF(len(CONFIG[\"label_list\"])).to(device)\n","optimizer = AdamW(model.parameters(), lr=best_lr, weight_decay=best_wd)\n","num_training_steps = best_max_ep * len(train_val) // best_bsz\n","scheduler = get_scheduler(\"linear\", optimizer=optimizer, num_warmup_steps=0, num_training_steps=num_training_steps)\n","\n","for epoch in range(1, best_max_ep + 1):\n","    model.train()\n","    total_loss = 0\n","    for batch in train_val_loader:\n","        input_ids = batch[\"input_ids\"].to(device)\n","        attention_mask = batch[\"attention_mask\"].to(device)\n","        labels = batch[\"labels\"].to(device)\n","        loss = model(input_ids, attention_mask, labels)\n","        loss_mean = loss.mean()\n","        total_loss += loss_mean.item()\n","        optimizer.zero_grad()\n","        loss_mean.backward()\n","        optimizer.step()\n","        scheduler.step()\n","\n","    avg_loss = total_loss / len(train_val_loader)\n","    print(f\"Эпоха {epoch} | Loss: {avg_loss:.4f}\")\n","\n","print(f\"\\n💾 Сохранение усреднённой модели на HF: {BERT_REPO_NAME}_mean\")\n","success = save_bert_to_hf(model, tokenizer, CONFIG, f\"{BERT_REPO_NAME}_mean\", HF_TOKEN)\n","if success:\n","    print(f\"🎉 BERT усреднённая модель успешно сохранена на HF\")\n","else:\n","    print(\"❌ Не удалось сохранить BERT усреднённую модель\")\n"]},{"cell_type":"code","source":["# === Стратегия 2: Ансамбль моделей ===\n","print(\"\\n=== Стратегия 2: Сохранение моделей фолдов для ансамбля ===\")\n","for i in range(6):\n","  success = save_fold_models_to_hf(fold_models, tokenizer, f'{BERT_REPO_NAME}_{i+1}', HF_TOKEN, is_spacy=False)\n","  if success:\n","      print(f\"🎉 Модели фолдов успешно сохранены на HF\")\n","  else:\n","      print(\"❌ Не удалось сохранить модели фолдов\")\n"],"metadata":{"id":"BpUWEO0A1H4X"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# === Тестирование ===\n","print(\"\\n=== ПРОВЕРКА ЗАГРУЗКИ И ТЕСТИРОВАНИЕ ===\")\n","# Стратегия 1: Тестирование усреднённой модели\n","print(\"\\n--- Тестирование усреднённой модели ---\")\n","loaded_model, loaded_tokenizer, loaded_config = load_bert_from_hf(f\"{BERT_REPO_NAME}_mean\", HF_TOKEN, device)\n","if loaded_model:\n","    test_text = \"молоко Простоквашино 2.5% 1л\"\n","    wrapper = HFWrapper(loaded_model, loaded_tokenizer, id2label=CONFIG[\"id2label\"])\n","    doc = wrapper(test_text)\n","    entities = [(ent.start_char, ent.end_char, ent.label_) for ent in doc.ents]\n","    print(f\"Тестовый текст: '{test_text}'\")\n","    print(f\"Извлеченные сущности: {entities}\")\n","    process_submission_bert(\n","        model=loaded_model,\n","        tokenizer=loaded_tokenizer,\n","        input_file=f\"{DATA_DIR}/submission.csv\",\n","        output_file=f\"{OUT_DIR}/submission_final.csv\",\n","        id2label=CONFIG[\"id2label\"]\n","    )\n","else:\n","    print(\"❌ Не удалось загрузить усреднённую модель\")\n"],"metadata":{"id":"4y-naEN71Mp_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","# Стратегия 2: Тестирование ансамбля\n","print(\"\\n--- Тестирование ансамбля моделей ---\")\n","loaded_fold_models = [load_bert_from_hf(f\"{BERT_REPO_NAME}_fold{i+1}\", HF_TOKEN, device)[0] for i in range(1, 6)]\n","loaded_fold_models = [m for m in loaded_fold_models if m is not None]\n","if loaded_fold_models:\n","    entities = ensemble_predict(loaded_fold_models, tokenizer, test_text, is_spacy=False, id2label=CONFIG[\"id2label\"])\n","    print(f\"Тестовый текст: '{test_text}'\")\n","    print(f\"Извлеченные сущности (ансамбль): {entities}\")\n","\n","    # Обработка submission для ансамбля\n","    df = pd.read_csv(f\"{DATA_DIR}/submission.csv\", sep=';')\n","    results = []\n","    for text in df['sample']:\n","        entities = ensemble_predict(loaded_fold_models, tokenizer, text, is_spacy=False, id2label=CONFIG[\"id2label\"])\n","        results.append(entities)\n","    output_df = pd.DataFrame({'sample': df['sample'], 'annotation': results})\n","    output_df.to_csv(f\"{OUT_DIR}/submission_ensemble.csv\", sep=';', index=False)\n","    print(f\"✅ Результаты ансамбля сохранены в: {OUT_DIR}/submission_ensemble.csv\")\n","else:\n","    print(\"❌ Не удалось загрузить модели фолдов\")"],"metadata":{"id":"0md5goGe6WnB"},"execution_count":null,"outputs":[]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}